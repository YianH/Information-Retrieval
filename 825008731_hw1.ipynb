{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 1:  Basic Machine Learning + Learning to Rank \n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, February 12 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get hands-on experience with (i) the basics of machine learning (e.g. train/test data, cross-validation, different classifiers) and interpreting results; and (ii) learning to rank.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, February 15 at 11:59pm.\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basics of ML (70 points)\n",
    "\n",
    "For this part, we're going to get familiar with scikit-learn (a great ML toolkit that is very popular) and the major issues in training a model, testing it, and interpreting the results. Our goal in this assignment is to build a classifier to determine if a Yelp review is \"food-relevant\" or not.\n",
    "\n",
    "## Dataset: Yelp review data\n",
    "\n",
    "First, you will need to download the training_data.json file from the Resources tab on Piazza, a collection of 40,000 json-encoded Yelp reviews we sampled from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "You'll see that each line corresponds to a review on a particular business. The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Parsing Yelp (15 points)\n",
    "\n",
    "For this first part, we will build a parser for extracting tokens from the **review text** only. First, you should tokenize each review using **whitespaces and punctuations as delimiters**. Do not remove stopwords. You should apply casefolding (lower case everything) and use the [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) ... you may need to install nltk if you don't have it already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of rows: 40000\n",
      "numbers of tokens: 5155527\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# use as many cells as you need\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import urllib2\n",
    "\n",
    "with open('/Users/huangyian/Desktop/training_data.json', 'r') as f:\n",
    "    file_lines = [''.join([x.strip(), ',', '\\n']) for x in f.readlines()]\n",
    "print 'numbers of rows:', len(file_lines)\n",
    "output = open('/Users/huangyian/Desktop/outfile.json', 'w')\n",
    "output.write('[')\n",
    "with output as f:\n",
    "    for i in range(0, len(file_lines) - 1):\n",
    "        f.writelines(file_lines[i]) \n",
    "    lastline = file_lines[len(file_lines) - 1].strip()[:-1]\n",
    "    f.writelines(lastline)\n",
    "    f.write(']')\n",
    "output.close() \n",
    "\n",
    "with open('/Users/huangyian/Desktop/outfile.json', 'r') as f:\n",
    "    allfile = json.load(f)\n",
    "st = PorterStemmer()\n",
    "eachReview = []\n",
    "tokenLists = []\n",
    "alltokens = []\n",
    "for review in allfile:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    eachReview = ''\n",
    "    for token in tokenizer.tokenize(review['text']):\n",
    "        word = st.stem(token.lower())\n",
    "        alltokens.append(word)\n",
    "        eachReview = eachReview + ' ' + word\n",
    "    tokenLists.append(eachReview)\n",
    "\n",
    "# alltokens list is all tokens extracted from the json file\n",
    "# print alltokens[:20]\n",
    "print 'numbers of tokens:', len(alltokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique tokens?\n",
    "\n",
    "Once you have your parser working, you should report here the size of your feature space. That is, how many unique tokens do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of unique tokens: 36618\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print len(alltokens)\n",
    "freqMap = nltk.FreqDist(alltokens)\n",
    "wordList = sorted(freqMap.iteritems(), key=lambda x : (x[1], x[0]), reverse = True)\n",
    "lenOfUniq = len(wordList)\n",
    "print 'numbers of unique tokens:', lenOfUniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Most Popular Words\n",
    "\n",
    "Great, now we can tokenize the documents. Let's make a list of the most popular words in our reviews. For this step, you should maintain a count of how many times each word occurs. Then you should print out the top-20 words in your reviews.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "Rank Token Count\n",
    "\n",
    "1 awesome 78\n",
    "\n",
    "... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 the 246309\n",
      "2 i 168930\n",
      "3 and 168589\n",
      "4 a 134898\n",
      "5 to 128139\n",
      "6 it 78867\n",
      "7 of 76237\n",
      "8 wa 74020\n",
      "9 is 63496\n",
      "10 for 60867\n",
      "11 in 60523\n",
      "12 that 50804\n",
      "13 my 50565\n",
      "14 you 45881\n",
      "15 they 43635\n",
      "16 thi 39940\n",
      "17 with 39340\n",
      "18 have 39082\n",
      "19 but 37967\n",
      "20 on 35386\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "top20List = wordList[:20]\n",
    "cfList = []\n",
    "index = 1\n",
    "for key, val in top20List:\n",
    "    cfList.append(val)\n",
    "    print index, key, val\n",
    "    index = index + 1\n",
    "# print cfList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "Recall in class our discussion of Zipf's law. Let's see if this law applies to our Yelp reviews. You should use matplotlib to plot the log-base10 term counts on the y-axis versus the log-base10 rank on the x-axis. Your aim is to create a figure like the one in Figure 5.2 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a28141050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "fList = []\n",
    "ind = 1\n",
    "for key, val in wordList:\n",
    "    fList.append(val)\n",
    "    ind = ind + 1\n",
    "    \n",
    "for i in range(1, len(wordList)):\n",
    "    x.append(math.log10(i))\n",
    "    \n",
    "for i in range(1, len(wordList)):\n",
    "    y.append(math.log10(fList[i]))\n",
    "    \n",
    "plt.plot(x, y)\n",
    "plt.xlabel('log10 rank')\n",
    "plt.ylabel('log10 cf')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Is this consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency decreases rapidly with rank, which is consistent with Zipf's law. It means that the frequency of any word is inversely proportional to its rank in the frequency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Feature Represenation (10 points)\n",
    "\n",
    "In this part you will build feature vectors for each review. This will be input to our ML classifiers. You should call your parser from earlier, using all the same assumptions (e.g., casefolding, stemming). Each feature value should be the term count for that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build feature vectors\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "featureVectors = vectorizer.fit_transform(tokenLists)\n",
    "featureVectors = featureVectors.toarray();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Machine Learning Basics (30 points)\n",
    "\n",
    "In this part you will evaluate a bunch of classifiers -- kNN, Decision tree, Naive Bayes, and SVM -- on the feature vectors generated in the previous task in two different settings. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.**\n",
    "\n",
    "### Setting 1: Splitting data into train-test \n",
    "\n",
    "In the first setting, you should treat the first 70% of your data as training. The remaining 30% should be for testing. \n",
    "\n",
    "### Setting 2: Using 5 fold cross-validation\n",
    "\n",
    "In the second setting, use 5-folk cross-validation. \n",
    "\n",
    "### What to report\n",
    "\n",
    "* Report the overall accuracy for both settings.\n",
    "* For the class \"Food-relevant\", report the precision and recall for both settings.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall for both settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set1:\n",
      "overall accuracy: 0.752380952381\n",
      "food-relevant precision: 0.791537050014\n",
      "food-irrelevant precision: 0.768714142071\n",
      "food-relevant recall: 0.902480144225\n",
      "food-irrelevant recall: 0.423737541985\n",
      "accuracy for decisionTreeClassifier, naiveBayesCls, knn, clfSVR: [0.77380952380952384, 0.794047619047619, 0.74285714285714288, 0.69880952380952377]\n",
      "Set2:\n",
      "overall accuracy: 0.726160714286\n",
      "food-relevant precision: 0.774177552739\n",
      "food-irrelevant precision: 0.722358636214\n",
      "food-relevant recall: 0.879724226769\n",
      "food-irrelevant recall: 0.388940599605\n",
      "accuracy for decisionTreeClassifier, naiveBayesCls, knn, clfSVR: [0.76357142857142857, 0.71285714285714286, 0.72464285714285714, 0.70357142857142863]\n"
     ]
    }
   ],
   "source": [
    "# your code here...plus add cells for reporting your results\n",
    "# https://stackoverflow.com/questions/27357121/scikit-calculate-precision-and-recall-using-cross-val-score-function\n",
    "# https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, precision_recall_fscore_support as score\n",
    "from sklearn import svm, preprocessing\n",
    "\n",
    "labels = []\n",
    "for item in allfile:\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "def classifierPerformSet1 (classifer, testPercentage, featureVectors, labels):\n",
    "    featureVectors_train, featureVectors_test, label_train, label_test = train_test_split(featureVectors, labels, test_size=testPercentage)\n",
    "    classifer.fit(featureVectors_train, label_train)\n",
    "    predicted=classifer.predict(featureVectors_test)\n",
    "    accuracy = accuracy_score(label_test, predicted)\n",
    "    precision, recall, fscore, support = score(label_test, predicted, labels=['Food-relevant','Food-irrelevant'])\n",
    "    \n",
    "    return accuracy,precision[0], precision[1], recall[0], recall[1]\n",
    "        \n",
    "def classifierPerformSet2 (classifer, foldNumber, featureVectors, labels):\n",
    "    \n",
    "    totalAccuracy = 0\n",
    "    totalPrecisionIf = 0\n",
    "    totalPrecisionRf = 0\n",
    "    totalRecallIF = 0\n",
    "    totalRecallRF = 0\n",
    "    \n",
    "    kf = KFold(n_splits=foldNumber)\n",
    "\n",
    "    for train_index, test_index in kf.split(featureVectors):\n",
    "        \n",
    "        featureVectors_train = featureVectors[train_index]\n",
    "        featureVectors_test = featureVectors[test_index]\n",
    "        label_train = labels[train_index]\n",
    "        label_test = labels[test_index]\n",
    "        \n",
    "        \n",
    "        classifer.fit(featureVectors_train, label_train)\n",
    "        predicted=classifer.predict(featureVectors_test)\n",
    "        precision, recall, fscore, support = score(label_test, predicted, labels=['Food-relevant','Food-irrelevant'])\n",
    "        \n",
    "        totalAccuracy += accuracy_score(label_test, predicted)\n",
    "        totalPrecisionRf += precision[0]\n",
    "        totalPrecisionIf += precision[1]\n",
    "        totalRecallRF += recall[0]\n",
    "        totalRecallIF += recall[1]\n",
    "    \n",
    "#     scores = cross_val_score(classifer, featureVectors, labels, cv=foldNumber, scoring='accuracy')\n",
    "#     precision, recall, fscore, support = score(label_test, predicted, labels=['Food-relevant','Food-irrelevant'])\n",
    "    return totalAccuracy/(foldNumber),totalPrecisionRf/(foldNumber),totalPrecisionIf/(foldNumber), totalRecallRF/(foldNumber), totalRecallIF/(foldNumber)\n",
    "\n",
    "def calculInformationForclassifierSet1 (classifierList):\n",
    "    \n",
    "    length = len(classifierList)\n",
    "    totalAccuracy = 0\n",
    "    totalFoodRelevantPrecision = 0\n",
    "    totalFoodIrrelevantPrecision = 0\n",
    "    totalFoodRelevantRecall = 0\n",
    "    totalFoodIrrelevantRecall=0\n",
    "    accuracylist = []\n",
    "    for classifier in classifierList:\n",
    "        accuracy, frPrecision, ifrPrecision, frRecall, ifrRecall = classifierPerformSet1(classifier, 0.3, featureVectors, labels)\n",
    "#         accuracySet2, precisionSet2, recallSet2 = calculateAccuracySet2(classifier, 5, featureVectors, label)\n",
    "        accuracylist.append(accuracy)\n",
    "        totalAccuracy += accuracy\n",
    "        totalFoodRelevantPrecision += frPrecision\n",
    "        totalFoodIrrelevantPrecision += ifrPrecision\n",
    "        totalFoodRelevantRecall += frRecall\n",
    "        totalFoodIrrelevantRecall += ifrRecall\n",
    "    \n",
    "    return totalAccuracy/length, totalFoodRelevantPrecision/length, totalFoodIrrelevantPrecision/length, totalFoodRelevantRecall/length, totalFoodIrrelevantRecall/length, accuracylist\n",
    "\n",
    "def calculInformationForclassifierSet2 (classifierList):\n",
    "    length = len(classifierList)\n",
    "    accuracylist = []\n",
    "    totalAccuracy = 0\n",
    "    totalFoodRelevantPrecision = 0\n",
    "    totalFoodIrrelevantPrecision = 0\n",
    "    totalFoodRelevantRecall = 0\n",
    "    totalFoodIrrelevantRecall=0\n",
    "    \n",
    "    \n",
    "    for classifier in classifierList:\n",
    "        accuracy, frPrecision, ifrPrecision, frRecall, ifrRecall = classifierPerformSet2(classifier, 5, featureVectors, labels)\n",
    "        accuracylist.append(accuracy)\n",
    "        totalAccuracy += accuracy\n",
    "        totalFoodRelevantPrecision += frPrecision\n",
    "        totalFoodIrrelevantPrecision += ifrPrecision\n",
    "        totalFoodRelevantRecall += frRecall\n",
    "        totalFoodIrrelevantRecall += ifrRecall\n",
    "    \n",
    "    return totalAccuracy/length, totalFoodRelevantPrecision/length, totalFoodIrrelevantPrecision/length, totalFoodRelevantRecall/length, totalFoodIrrelevantRecall/length, accuracylist\n",
    "\n",
    "\n",
    "\n",
    "decisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "naiveBayesCls = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "clfSVR = svm.SVC()\n",
    "\n",
    "# print classifierPerformSet1(decisionTreeClassifier, 0.3, featureVectors, label)\n",
    "# print classifierPerformSet2(decisionTreeClassifier, 5, featureVectors, label)\n",
    "# # print labels\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# print lb.fit_transform(labels)\n",
    "# labels = np.array([number[0] for number in lb.fit_transform(labels)])\n",
    "# print labels\n",
    "# scores = cross_validation.cross_val_score(knn, featureVectors, labels, cv=5, scoring='recall')\n",
    "# print scores.mean()\n",
    "\n",
    "classifierList = [decisionTreeClassifier, naiveBayesCls, knn, clfSVR]\n",
    "resultSet1 = calculInformationForclassifierSet1(classifierList)\n",
    "print 'Set1:'\n",
    "print 'overall accuracy:', resultSet1[0]\n",
    "print 'food-relevant precision:', resultSet1[1]\n",
    "print 'food-irrelevant precision:',resultSet1[2]\n",
    "print 'food-relevant recall:',resultSet1[3]\n",
    "print 'food-irrelevant recall:',resultSet1[4]\n",
    "print 'accuracy for decisionTreeClassifier, naiveBayesCls, knn, SVM:', resultSet1[5]\n",
    "\n",
    "resultSet2 = calculInformationForclassifierSet2(classifierList)\n",
    "print 'Set2:'\n",
    "print 'overall accuracy:', resultSet2[0]\n",
    "print 'food-relevant precision:', resultSet2[1]\n",
    "print 'food-irrelevant precision:', resultSet2[2]\n",
    "print 'food-relevant recall:', resultSet2[3]\n",
    "print 'food-irrelevant recall:', resultSet2[4]\n",
    "print 'accuracy for decisionTreeClassifier, naiveBayesCls, knn, SVM:',resultSet2[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Analyzing your results (5 points) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those 4 classifiers, SVM with default parameters has the worst accuracy, and the decision tree and naive bayes have better accuracy. But the classifiers I choose in this problem are all created with default parameters. However, if parameters are changed, the accuracy will be totally different. For example, if C = 100 and gamma = 0.0001 for SVM, the accuracy will be improved. Also, we can altered the numbers of neighbors of KNN approach to achieve a better result. \n",
    "\n",
    "Then we compare these 2 settings, the overall accuracy of setting 1 is better than setting 2 and also the perspective accuracy of KNN, decision tree, naive bayes and SVM. As for the precision and recall, setting 1 achieves higher precision and recall for both food-relevant and food-irrelevant classes, which mean train-test data returns substantially more relevant results than irrelevant ones(precision) and returns most of the relevant results(recall) comparing with 5-cross-validation setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Improving your classifier (10 points)\n",
    "\n",
    "I think we can do better! In this part, your job is to create new features that you can think can help improve your classifier. You may choose to use new weightings for your words, new derived features (e.g., count of 3-letter words), or whatever you like. You may also add in the extra features in the json: funny, useful, cool. You will need to experiment with different approaches ... once you finalize on your best approach, include the features here with a description (that is, tell us what the feature means). Then give us your classifier results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here ... add as many cells as you need for features, results, and discussion.\n",
    "# I tried to change the K in KNN, C and gamma in SVM, below are some tests code, and I will put the code of best result\n",
    "# in the next code cell.\n",
    "# clf = svm.SVC(gamma = 0.0001, C=100)\n",
    "# origin = [clf]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet1(origin)[5]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet1(origin)[5]\n",
    "# clf1 = svm.SVC(gamma = 0.0001, C=100)\n",
    "# test = [dt1, nb1, k1, clf1]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet1(test)[5]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet1(test)[5]\n",
    "\n",
    "# k = KNeighborsClassifier()\n",
    "# origin = [k]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet1(origin)[5]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet1(origin)[5]\n",
    "# k1 = KNeighborsClassifier(n_neighbors = 10)\n",
    "# test = [k1]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet2(test)[5]\n",
    "# print 'SVM accuracy:', calculInformationForclassifierSet2(test)[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disscuss:\n",
    "In this section, I tried to change some parameters of the classifiers, such as the K in KNN, and also gamma and C in SVM approach. No matter how I changed K, the accuracy of KNN does not change too much. For gamma and C in SVM, these 2 parameters could somewhat affect the result. \n",
    "\n",
    "Also, I tried to change the way of creating feature vectors. In the previous section, we use countVectorizer method and each feature value should be the term count for that review. However, we did not consider the effect of stop words and thus effect our final results. Stop words were always the top words, but these words will give us useless information and distract us from the right result. On the contrary, words like 'breakfast', 'lunch', 'dinner', 'eat', 'delicious', 'tasty' and so on should be weighted more. I used TFIDFVectorizer to improve the performance. This can reflect how important a word is to a document in a collection and can be used as a weighting factor and also used for stop-words filtering in various subject fields. The result show that the total accuracy have improved and KNN accuracy improved significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set1:\n",
      "overall accuracy: 0.791071428571\n",
      "food-relevant precision: 0.826057825808\n",
      "food-irrelevant precision: 0.55210868631\n",
      "food-relevant recall: 0.902666183866\n",
      "food-irrelevant recall: 0.535933999235\n",
      "accuracy for decisionTreeClassifier, naiveBayesCls, knn, clfSVR: [0.79523809523809519, 0.77500000000000002, 0.89523809523809528, 0.69880952380952377]\n",
      "Set2:\n",
      "overall accuracy: 0.758660714286\n",
      "food-relevant precision: 0.800320282047\n",
      "food-irrelevant precision: 0.509646138763\n",
      "food-relevant recall: 0.885994662725\n",
      "food-irrelevant recall: 0.479779040404\n",
      "accuracy for decisionTreeClassifier, naiveBayesCls, knn, clfSVR: [0.77250000000000008, 0.72357142857142853, 0.84892857142857159, 0.68964285714285711]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transformer = TfidfVectorizer()\n",
    "tfidf = transformer.fit_transform(tokenLists)\n",
    "altered_featureVectors = tfidf.toarray()\n",
    "def calculInformationForclassifierSet1 (classifierList):\n",
    "    \n",
    "    length = len(classifierList)\n",
    "    totalAccuracy = 0\n",
    "    totalFoodRelevantPrecision = 0\n",
    "    totalFoodIrrelevantPrecision = 0\n",
    "    totalFoodRelevantRecall = 0\n",
    "    totalFoodIrrelevantRecall=0\n",
    "    accuracylist = []\n",
    "    for classifier in classifierList:\n",
    "        accuracy, frPrecision, ifrPrecision, frRecall, ifrRecall = classifierPerformSet1(classifier, 0.3, altered_featureVectors, labels)\n",
    "#         accuracySet2, precisionSet2, recallSet2 = calculateAccuracySet2(classifier, 5, featureVectors, label)\n",
    "        accuracylist.append(accuracy)\n",
    "        totalAccuracy += accuracy\n",
    "        totalFoodRelevantPrecision += frPrecision\n",
    "        totalFoodIrrelevantPrecision += ifrPrecision\n",
    "        totalFoodRelevantRecall += frRecall\n",
    "        totalFoodIrrelevantRecall += ifrRecall\n",
    "    \n",
    "    return totalAccuracy/length, totalFoodRelevantPrecision/length, totalFoodIrrelevantPrecision/length, totalFoodRelevantRecall/length, totalFoodIrrelevantRecall/length, accuracylist\n",
    "\n",
    "def calculInformationForclassifierSet2 (classifierList):\n",
    "    length = len(classifierList)\n",
    "    accuracylist = []\n",
    "    totalAccuracy = 0\n",
    "    totalFoodRelevantPrecision = 0\n",
    "    totalFoodIrrelevantPrecision = 0\n",
    "    totalFoodRelevantRecall = 0\n",
    "    totalFoodIrrelevantRecall=0\n",
    "    \n",
    "    \n",
    "    for classifier in classifierList:\n",
    "        accuracy, frPrecision, ifrPrecision, frRecall, ifrRecall = classifierPerformSet2(classifier, 5, altered_featureVectors, labels)\n",
    "        accuracylist.append(accuracy)\n",
    "        totalAccuracy += accuracy\n",
    "        totalFoodRelevantPrecision += frPrecision\n",
    "        totalFoodIrrelevantPrecision += ifrPrecision\n",
    "        totalFoodRelevantRecall += frRecall\n",
    "        totalFoodIrrelevantRecall += ifrRecall\n",
    "    \n",
    "    return totalAccuracy/length, totalFoodRelevantPrecision/length, totalFoodIrrelevantPrecision/length, totalFoodRelevantRecall/length, totalFoodIrrelevantRecall/length, accuracylist\n",
    "\n",
    "\n",
    "\n",
    "df = DecisionTreeClassifier(random_state=0)\n",
    "nb = GaussianNB()\n",
    "k = KNeighborsClassifier()\n",
    "clf = svm.SVC(C = 100, gamma = 0.0001)\n",
    "classifierList = [dt, nb, k, clf]\n",
    "improvedResult = calculInformationForclassifierSet1(classifierList)\n",
    "print 'Set1:'\n",
    "print 'overall accuracy:', improvedResult[0]\n",
    "print 'food-relevant precision:', improvedResult[1]\n",
    "print 'food-irrelevant precision:',improvedResult[2]\n",
    "print 'food-relevant recall:',improvedResult[3]\n",
    "print 'food-irrelevant recall:',improvedResult[4]\n",
    "print 'accuracy for decisionTreeClassifier, naiveBayesCls, knn, SVM:', improvedResult[5]\n",
    "\n",
    "improvedResult2 = calculInformationForclassifierSet2(classifierList) \n",
    "print 'Set2:'\n",
    "print 'overall accuracy:', improvedResult2[0]\n",
    "print 'food-relevant precision:', improvedResult2[1]\n",
    "print 'food-irrelevant precision:', improvedResult2[2]\n",
    "print 'food-relevant recall:', improvedResult2[3]\n",
    "print 'food-irrelevant recall:', improvedResult2[4]\n",
    "print 'accuracy for decisionTreeClassifier, naiveBayesCls, knn, SVM:', improvedResult2[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: What are the most informative features in distinguishing these two classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re-weight term count is the most informative features in distinguishing these two classes. We could extract the more interesting and valueable terms instead of very frequent words, such as the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Learning to Rank (30 points)\n",
    "\n",
    "For this part, we're going to play with some Microsoft LETOR data that has query-document relevance judgments. Let's see how learning to rank works in practice. \n",
    "\n",
    "First, you will need to download the MQ2008.zip file from the Resources tab on Piazza. This is data from the [Microsoft Research IR Group](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/).\n",
    "\n",
    "The data includes 15,211 rows. Each row is a query-document pair. The first column is a relevance label of this pair (0,1 or 2--> the higher value the more related), the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. A query-document pair is represented by a 46-dimensional feature vector. Features are a numeric value describing a document and query such as TFIDF, BM25, Page Rank, .... You can find compelete description of features from [here](https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf).\n",
    "\n",
    "The good news for you is the dataset is ready for analysis: It has already been split into 5 folds (see the five folders called Fold1, ..., Fold5).\n",
    "\n",
    "For this assignment, we're going to leave our favorite scikit-learn and instead use [SVM-rank](https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html). This is the basic ranking SVM we talked about in class. You'll see that SVM-rank considers pairwise relevance between docs -- so based on the training data it will transform the data into pairs -- like D1 > D2 and then learn a separator.\n",
    "\n",
    "\n",
    "## Part 2.1: Optimizing SVM-Rank (15 points)\n",
    "\n",
    "First, you should explore how the different parameters affect the quality of the Ranking SVM. You'll see that you can vary the kernel function, the loss function and so forth. \n",
    "\n",
    "You should run SVM-Rank using the default options over each of the five folds. You should find the error on the test set (for example, depending on your settings, svm_rank_classify will give you the zero/one error statistics (that is, the number of correct pairs and the number of incorrect pairs). Report the average. \n",
    "\n",
    "Then try different parameters and report how they impact the quality of results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the default setting (trade-off between training error and margin = default 0.01)\n",
    "For Folder1: 58.33%\n",
    "For Folder2: 56.05%\n",
    "For Folder3: 62.42%\n",
    "For Folder4: 70.06%\n",
    "For Folder5: 64.33%\n",
    "the average of the zero/one-error on test set is (58.33% + 56.05% + 62.42% + 70.06% + 64.33%) / 5 = 62.238%\n",
    "\n",
    "When trying to change the trade-off between training error and margin = 1, the zero/one-error average decreased:\n",
    "Average = (58.33% + 55.41% + 61.78% + 66.88% + 64.97%) / 5 = 61.47%\n",
    "\n",
    "When trying to change the loss functions = 2(which means the fraction of swapped pairs averaged over all queries), the average of zero/one-error on test sets decreased a bit:\n",
    "Average = (57.05% + 54.14% + 61.78% + 73.25% + 64.33% ) / 5 = 62.11%\n",
    "\n",
    "When trying to change the Optimization Options with 1-slack algorithm (dual) with constraint cache,\n",
    "average of zero/one-error on test sets decreased a bit:\n",
    "Average = (58.97% + 56.05% + 62.42% + 68.79% + 64.33%) / 5 = 62.112%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2.1: Noise! (15 points)\n",
    "\n",
    "Now we're going to investigate whether the ranking SVM is easily influenced by noisy features. For example, what if some of the features you have are in error? Or what if you downloaded only a portion of a page to calculate a feature? (so the count of inlinks would be wrong)? \n",
    "\n",
    "In this case, add some noise to the features. What happens to the results? You may choose to add random noise throughout, noise to a single feature, noise to multiple features, etc. The choices are up to you. We aim to see what kind of exploration you conduct and what you conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*add your results and discussion here*\n",
    "\n",
    "Discuss:\n",
    "For each folder in MQ2008, I randomly chose qids and added random noise to their features. At first, I only chose 50 items and add noise to all features, but the zero/one-error did not change, so I keep adding the noise to the features. The result below is randomly 500 items with noise on all features:\n",
    "\n",
    "As we can see from the zero/one-error on the 5 folders, the average = (57.69% + 56.05% + 62.42% + 70.70% + 63.69%) / 5 = 62.11%, which decreased by 0.12% from the noise-free one. Also, the total number of swapped pairs and the average loss varied in each noise-added test-train pairs, but not in a significant way.\n",
    "\n",
    "I assume it is because SVM is robust in high dimensional spaces. Also, SVM has good generalization ability, so that only a small fraction of noise will not affect the accuracy. But the accuracy will definetely decrease if the noise is really noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*\n",
    "\n",
    "Declarations:\n",
    "https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer: use to tokenize words with Regex\n",
    "http://www.nltk.org/book_1ed/ch03.html to stem the words\n",
    "https://stackoverflow.com/questions/27357121/scikit-calculate-precision-and-recall-using-cross-val-score-function to calculate precision and recall \n",
    "https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/ some guides towards classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
