{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 3:  Embeddings + Recommenders\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, April 9 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* There are two main learning objectives: (i) implement and evaluate a pre-cursor to modern word2vec embeddings; and (ii) implement, evaluate, and improve upon traditional collaborative filtering recommenders.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw3.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, April 12 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Embeddings (50 points)\n",
    "For this first part, we're going to implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters.\n",
    "\n",
    "If you're interested in a deeper treatment of comparing count vs. learned embeddings, take a look at: [Donâ€™t count, predict! A systematic comparison of\n",
    "context-counting vs. context-predicting semantic vectors](\n",
    "http://www.aclweb.org/anthology/P14-1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us, you can find the Brown corpus in nltk. *Make sure you have already installed nltk with something like: conda install nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/huangyian/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/huangyian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it locally, you can load the dataset into your notebook. You can access the words using brown.words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 in the list: [u'one', u'would', u'said', u'time', u'new', u'could', u'two', u'may', u'first', u'man', u'like', u'even', u'made', u'also', u'many', u'must', u'well', u'af', u'back', u'years']\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_list = brown.words()\n",
    "stop = set(stopwords.words('english')) \n",
    "preprocessed_list = [x.lower() for x in word_list]\n",
    "\n",
    "preprocessed_list = [tokenizer.tokenize(word) for word in preprocessed_list]\n",
    "\n",
    "dimensionConv = []\n",
    "for listWord in preprocessed_list:\n",
    "    for word in listWord:\n",
    "        dimensionConv.append(word)\n",
    "preprocessed_list = dimensionConv\n",
    "preprocessed_list = [word for word in preprocessed_list if word not in stop]\n",
    "\n",
    "# preprocessed = []\n",
    "# for w in preprocessed_list:\n",
    "#     if re.match('^[a-zA-Z0-9]*$', w):\n",
    "#         if w not in stop:\n",
    "#             preprocessed.append(w)\n",
    "# preprocessed_list = preprocessed\n",
    "\n",
    "freqMap = nltk.FreqDist(preprocessed_list)\n",
    "freqList = sorted(freqMap.iteritems(), key=lambda x : (x[1], x[0]), reverse = True)\n",
    "wordList = []\n",
    "countFre = []\n",
    "for key, val in freqList:\n",
    "    countFre.append(val)\n",
    "    wordList.append(key)\n",
    "print \"top 20 in the list:\", wordList[:20]\n",
    "V = wordList[:5000]\n",
    "C = wordList[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "import numpy\n",
    "len_v, len_c = len(V), len(C)\n",
    "print len_v\n",
    "len_pre = len(preprocessed_list)\n",
    "co_occur = [[0 for x in range(len_c)] for y in range(len_v)]\n",
    "# print co_occur\n",
    "for index in range(0, len_pre):\n",
    "    preprocessed_words = preprocessed_list[index]\n",
    "    if preprocessed_words in V:\n",
    "        row = V.index(preprocessed_words)    \n",
    "        left = index - 2;\n",
    "        right = index + 2;\n",
    "        if right >= len_pre:\n",
    "            right = len_pre - 1;\n",
    "        if left < 0:\n",
    "            left = 0;\n",
    "            \n",
    "        for inner in range(left, right + 1):\n",
    "            word = preprocessed_list[inner]\n",
    "            if (inner != index) & (word in C):\n",
    "                column = C.index(word) \n",
    "                co_occur[row][column] += 1\n",
    "#         for ind_c in range(0, len_c):\n",
    "#             for ind_v in range(left, right + 1):\n",
    "#                 if C[ind_c] == preprocessed_list[ind_v]:\n",
    "#                     co_occur[row][ind_c] = co_occur[row][ind_c] + 1\n",
    "        \n",
    "print \"end\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "import numpy as np\n",
    "print len_v\n",
    "# Pr(c|w)\n",
    "conditional_matrix = [[0 for x in range(len_c)] for y in range(len_v)]\n",
    "for i in range(0, len_v):\n",
    "    sumup = 0\n",
    "    for j in range(0, len_c):\n",
    "        sumup = sumup + co_occur[i][j]\n",
    "    for j in range(0, len_c):\n",
    "        if sumup == 0:\n",
    "            conditional_matrix[i][j] = 0\n",
    "        else:\n",
    "            conditional_matrix[i][j] = float(co_occur[i][j]) / sumup\n",
    "\n",
    "# Pr(c)\n",
    "occurence_c = []\n",
    "for j in range(0, len_c):\n",
    "    sumup = 0\n",
    "    for i in range(0, len_v):\n",
    "        sumup = sumup + co_occur[i][j]\n",
    "    occurence_c.append(sumup)\n",
    "    \n",
    "total = 0;\n",
    "pr_c = []\n",
    "for j in range(0, len_c):\n",
    "    total = total + occurence_c[j]\n",
    "for j in range(0, len_c):\n",
    "    pr_c.append(float(occurence_c[j]) / total)\n",
    "    \n",
    "# print conditional_matrix   \n",
    "# print pr_c\n",
    "print \"end\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now you can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "import math\n",
    "c = 0\n",
    "vector_w = [[0 for x in range(len_c)] for y in range(len_v)]\n",
    "for i in range(0, len_v):\n",
    "    for j in range(0, len_c):\n",
    "        if conditional_matrix[i][j] == 0:\n",
    "            vector_w[i][j] = 0\n",
    "            c = c + 1\n",
    "        else:\n",
    "            vector_w[i][j] = max(0, math.log(float(conditional_matrix[i][j]) / pr_c[j]))\n",
    "print len_v\n",
    "print len(vector_w)\n",
    "# print vector_w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, you should:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# # Your Code Here...\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import spatial\n",
    "top20 = wordList[:20]\n",
    "vec20 = vector_w[:20]\n",
    "word_vec = np.array(vector_w)\n",
    "km = KMeans(n_clusters=100, random_state=0).fit_predict(word_vec)\n",
    "print \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 : [u'alexander']\n",
      "Cluster 2 : [u'acquired']\n",
      "Cluster 3 : [u'pulled']\n",
      "Cluster 4 : [u'arnold', u'empire']\n",
      "Cluster 5 : [u'children', u'education', u'schools', u'subjects']\n",
      "Cluster 6 : [u'company', u'industrial', u'manager']\n",
      "Cluster 7 : [u'pursue']\n",
      "Cluster 8 : [u'figure']\n",
      "Cluster 9 : [u'even', u'made', u'many', u'must', u'well', u'much', u'people', u'make', u'work', u'life', u'another', u'might', u'great', u'since', u'without', u'found', u'part', u'upon', u'every', u'course', u'always', u'fact', u'though', u'far', u'called', u'almost', u'yet', u'better', u'present', u'point', u'find', u'second', u'group', u'social', u'give', u'order', u'important', u'rather', u'case', u'among', u'often', u'god', u'best', u'need', u'church', u'become', u'power', u'family', u'least', u'seemed', u'mind', u'today', u'help', u'others', u'although', u'law', u'whole', u'problem', u'sense', u'kind', u'certain', u'thus', u'name', u'matter', u'perhaps', u'human', u'action', u'free', u'show', u'example', u'history', u'whether', u'death', u'gave', u'either', u'act', u'quite', u'word', u'seen', u'money', u'experience', u'words', u'class', u'college', u'already', u'making', u'shall', u'known', u'political', u'real', u'probably', u'seems', u'question', u'century', u'cannot', u'brought', u'individual', u'whose', u'child', u'self', u'became', u'short', u'position', u'age', u'reason', u'society', u'love', u'community', u'true', u'seem', u'future', u'clear', u'common', u'women', u'sometimes', u'music', u'third', u'students', u'able', u'effect', u'art', u'usually', u'plan', u'book', u'therefore', u'evidence', u'english', u'strong', u'living', u'believe', u'says', u'modern', u'mean', u'process', u'personal', u'longer', u'alone', u'situation', u'idea', u'nature', u'private', u'finally', u'view', u'spirit', u'person', u'except', u'recent', u'particular', u'attention', u'live', u'hope', u'middle', u'beyond', u'read', u'earth', u'story', u'fine', u'feeling', u'lost', u'instead', u'makes', u'simply', u'picture', u'simple', u'religious', u'actually', u'sort', u'received', u'beginning', u'friends', u'subject', u'indeed', u'especially', u'difficult', u'paper', u'bring', u'natural', u'written', u'working', u'final', u'non', u'purpose', u'issue', u'answer', u'needs', u'likely', u'considered', u'meet', u'french', u'thinking', u'difference', u'color', u'involved', u'christian', u'particularly', u'knowledge', u'reading', u'ideas', u'deal', u'certainly', u'moral', u'statement', u'showed', u'questions', u'neither', u'science', u'student', u'strength', u'understand', u'degree', u'comes', u'trial', u'merely', u'concerned', u'direction', u'literature', u'freedom', u'influence', u'cause', u'meaning', u'works', u'ways', u'theory', u'fear', u'lead', u'truth', u'movement', u'clearly', u'note', u'negro', u'groups', u'forms', u'consider', u'placed', u'apparently', u'born', u'performance', u'immediately', u'approach', u'writing', u'understanding', u'persons', u'opportunity', u'served', u'progress', u'decision', u'religion', u'character', u'serious', u'christ', u'poor', u'justice', u'appear', u'account', u'nuclear', u'choice', u'obviously', u'gives', u'doubt', u'plans', u'established', u'whatever', u'speak', u'race', u'language', u'faith', u'completely', u'effects', u'principle', u'existence', u'elements', u'stress', u'scene', u'none', u'importance', u'expect', u'role', u'patient', u'follow', u'easily', u'attitude', u'professional', u'suggested', u'style', u'jazz', u'interested', u'despite', u'becomes', u'status', u'reasons', u'exactly', u'unless', u'raised', u'events', u'places', u'popular', u'knows', u'primary', u'parents', u'opinion', u'institutions', u'concern', u'claim', u'film', u'books', u'accepted', u'usual', u'success', u'giving', u'considerable', u'churches', u'behavior', u'tradition', u'successful', u'marriage', u'changed', u'attempt', u'americans', u'practice', u'poetry', u'highly', u'discussion', u'remain', u'dance', u'obvious', u'laws', u'noted', u'frequently', u'entirely', u'condition', u'catholic', u'caused', u'relationship', u'regard', u'goes', u'failure', u'develop', u'broad', u'moreover', u'greatest', u'sex', u'possibility', u'facts', u'takes', u'significant', u'shape', u'philosophy', u'otherwise', u'marked', u'allowed', u'stated', u'rules', u'musical', u'crisis', u'concept', u'teacher', u'remains', u'learn', u'jewish', u'enter', u'created', u'circumstances', u'begin', u'aware', u'appears', u'poems', u'offered', u'interests', u'agreed', u'youth', u'presented', u'mentioned', u'interesting', u'instance', u'lives', u'immediate', u'event', u'recognized', u'reality', u'offer', u'master', u'literary', u'fully', u'features', u'studied', u'secret', u'expression', u'desire', u'writers', u'trust', u'traditional', u'tone', u'mission', u'favor', u'response', u'writer', u'presence', u'expressed', u'difficulty', u'standards', u'rule', u'powers', u'pointed', u'familiar', u'faculty', u'contrast', u'brief', u'birth', u'ability', u'nevertheless', u'intellectual', u'individuals', u'command', u'assumed', u'advantage', u'independent', u'evil', u'detail', u'accept', u'universe', u'unity', u'responsible', u'proved', u'principles', u'historical', u'chosen', u'willing', u'teachers', u'seek', u'realized', u'politics', u'naturally', u'danger', u'calls', u'appeal', u'practical', u'largely', u'families', u'connection', u'statements', u'leading', u'excellent', u'emotional', u'characteristic', u'career', u'actions', u'communication', u'background', u'speaking', u'significance', u'search', u'rapidly', u'objects', u'issues', u'helped', u'discussed', u'capable', u'besides', u'artist', u'typical', u'powerful', u'contact', u'acting', u'spiritual', u'message', u'matters', u'highest', u'dramatic', u'contemporary', u'belief', u'aspects', u'conscious', u'assume', u'struggle', u'speech', u'setting', u'recognize', u'greatly', u'extreme', u'equally', u'concerning', u'achieved', u'vast', u'showing', u'possibly', u'ideal', u'evident', u'bible', u'artists', u'understood', u'technique', u'sensitive', u'learning', u'culture', u'critical', u'unique', u'ultimate', u'sexual', u'parties', u'finds', u'fiction', u'exist', u'conclusion', u'academic', u'vision', u'grounds', u'fairly', u'exercise', u'causes', u'truly', u'organized', u'majority', u'hardy', u'details', u'becoming', u'apparent', u'vital', u'regarded', u'identity', u'cultural', u'begins', u'risk', u'element', u'decisions', u'create', u'content', u'absence', u'protestant', u'experiences', u'examples', u'conflict', u'situations', u'necessarily', u'acceptance', u'fundamental', u'conviction', u'changing', u'partly', u'formal', u'creative', u'consideration', u'integration', u'attitudes', u'seeking', u'patterns', u'aspect', u'universal', u'seriously', u'origin', u'intended', u'recognition', u'opinions', u'lies', u'considerably', u'identification', u'environment', u'dealing', u'psychological', u'exists', u'civilization', u'societies', u'conscience', u'worship', u'regardless', u'readers', u'myth', u'tends', u'genuine', u'ethical']\n",
      "Cluster 10 : [u'earnings', u'describes']\n",
      "Cluster 11 : [u'missouri', u'maryland']\n",
      "Cluster 12 : [u'walked']\n",
      "Cluster 13 : [u'articles', u'carleton', u'publications', u'bulletin']\n",
      "Cluster 14 : [u'u']\n",
      "Cluster 15 : [u'phenomenon']\n",
      "Cluster 16 : [u'soldier']\n",
      "Cluster 17 : [u'october']\n",
      "Cluster 18 : [u'panic']\n",
      "Cluster 19 : [u'appearance']\n",
      "Cluster 20 : [u'000', u'300']\n",
      "Cluster 21 : [u'mike', u'beside', u'threw', u'deegan', u'shaking', u'grabbed']\n",
      "Cluster 22 : [u'af', u'points', u'plane', u'feed', u'image', u'aj', u'function', u'positive', u'fixed', u'file', u'double', u'cells', u'index', u'procedure', u'address', u'treated', u'follows', u'anode', u'cell', u'platform', u'column', u'text', u'sections', u'properties', u'measured', u'experiment', u'contained', u'tension', u'associated', u'formula', u'dictionary', u'previously', u'neighborhood', u'hence', u'etc', u'sample', u'requires', u'components', u'q', u'stream', u'slight', u'prove', u'engine', u'assigned', u'angle', u'characteristics', u'chain', u'stages', u'operator', u'liquid', u'functions', u'concentration', u'relative', u'precision', u'containing', u'bond', u'curve', u'contain', u'x', u'thickness', u'proof', u'listed', u'removal', u'recorded', u'readily', u'shift', u'reduction', u'particles', u'obtain', u'distinct', u'defined', u'axis', u'output', u'arc', u'shear', u'hydrogen', u'definition', u'combined', u'trials', u'represents', u'random', u'multiple', u'experimental', u'corresponding', u'contains', u'variable', u'stored', u'staining', u'gyro', u'coating', u'similarly', u'pencil', u'meets', u'isolated', u'binomial', u'tape', u'sequence', u'probability', u'density', u'calculated', u'ray', u'fiber', u'chlorine', u'stems', u'scheme', u'namely', u'equation', u'zg', u'tube', u'locking', u'occurrence', u'network', u'magnitude', u'identical', u'consequently', u'allowing', u'voting', u'sheets', u'electron', u'dimensions', u'solutions', u'velocity', u'sensitivity', u'polynomial', u'passes', u'optimal', u'compare', u'zero', u'valued', u'occurs', u'minimal', u'define', u'variables', u'tangent', u'outcome', u'magnetic', u'component', u'transformed', u'input', u'correspondence', u'vacuum', u'successes', u'substrate', u'displacement', u'category', u'reliable', u'particle', u'linear', u'dynamic', u'channel', u'arbitrary', u'masses', u'interval', u'intermediate', u'computed', u'cellulose', u'transformation', u'slope', u'probabilities', u'bundle', u'vertex', u'vector', u'representation']\n",
      "Cluster 23 : [u'straightened']\n",
      "Cluster 24 : [u'arts']\n",
      "Cluster 25 : [u'poetic', u'hamilton']\n",
      "Cluster 26 : [u'shoulders', u'knees']\n",
      "Cluster 27 : [u'district']\n",
      "Cluster 28 : [u'king']\n",
      "Cluster 29 : [u'services']\n",
      "Cluster 30 : [u'took', u'taken']\n",
      "Cluster 31 : [u'sets']\n",
      "Cluster 32 : [u'floor']\n",
      "Cluster 33 : [u'two', u'years', u'three', u'four', u'five', u'several', u'feet', u'six', u'ago', u'minutes', u'ten', u'hundred', u'twenty', u'miles', u'seven', u'eight', u'couple', u'thirty', u'nine', u'thousand', u'dollars', u'fifty', u'forty', u'fifteen', u'dollar', u'twelve', u'sixty', u'eleven', u'eighty', u'nineteen']\n",
      "Cluster 34 : [u'judgment']\n",
      "Cluster 35 : [u'payment', u'payments']\n",
      "Cluster 36 : [u'1957']\n",
      "Cluster 37 : [u'new', u'school', u'york', u'university', u'county', u'club', u'post', u'co', u'played', u'park', u'bank', u'san', u'providence', u'virginia', u'hearing', u'removed', u'village', u'texas', u'hudson', u'headquarters', u'coast', u'shore', u'officials', u'dallas', u'bay', u'georgia', u'lake', u'joined', u'fort', u'theater', u'lincoln', u'grand', u'philadelphia', u'los', u'ill', u'assembly', u'angeles', u'welcome', u'issued', u'hills', u'thousands', u'native', u'mobile', u'pennsylvania', u'avenue', u'nearby', u'manchester', u'includes', u'ships', u'route', u'orleans', u'hundreds', u'francisco', u'tour', u'mountains', u'illinois', u'heads', u'waters', u'mississippi', u'visited', u'downtown', u'democrats', u'harbor', u'atlanta', u'ohio', u'football', u'visiting', u'sixth', u'prince', u'elected', u'virgin', u'urged', u'returning', u'mills', u'harvard', u'candidate', u'gallery', u'filed', u'pacific', u'mountain', u'fishing', u'falls', u'factory', u'dartmouth', u'charter', u'campus', u'admission', u'temporary', u'museum', u'kansas', u'graduate', u'farther', u'territory', u'star', u'heading', u'yankees', u'louisiana', u'brooklyn', u'trips', u'residence', u'newport', u'jurisdiction', u'irish', u'extending', u'veteran', u'shopping', u'greenwich', u'weekend', u'voted', u'trustees', u'ranch', u'mount', u'jersey', u'honored', u'hollywood', u'delaware', u'1953', u'settlement', u'portland', u'invited', u'carolina', u'bridges', u'stadium', u'rank', u'northwest', u'miami', u'historic', u'workshop', u'tennessee', u'orange', u'neighboring', u'madison', u'factories', u'crossing', u'bore', u'baltimore', u'advisory', u'academy', u'vermont', u'traveled', u'memorial', u'gulf', u'civic', u'champion', u'strategy', u'port', u'mexico', u'florida', u'engagement', u'detroit', u'congressional', u'capitol', u'warwick', u'submitted', u'sixteen', u'residents', u'prairie', u'paula', u'monument', u'michigan', u'manhattan', u'jail', u'chapel', u'cape', u'wisconsin', u'voters', u'inc', u'gather', u'crawled', u'columbia', u'border', u'alabama', u'traveling', u'supervision']\n",
      "Cluster 38 : [u'training']\n",
      "Cluster 39 : [u'oxygen', u'pond', u'oxidation']\n",
      "Cluster 40 : [u'expansion']\n",
      "Cluster 41 : [u'state', u'world', u'states', u'american', u'war', u'united', u'government', u'president', u'city', u'country', u'south', u'west', u'department', u'court', u'party', u'policy', u'america', u'washington', u'north', u'peace', u'east', u'union', u'nation', u'return', u'nations', u'report', u'central', u'kennedy', u'administration', u'foreign', u'england', u'congress', u'western', u'countries', u'entire', u'anti', u'trade', u'throughout', u'soviet', u'southern', u'army', u'led', u'continued', u'former', u'efforts', u'press', u'recently', u'europe', u'british', u'reported', u'staff', u'communist', u'officer', u'democratic', u'attack', u'corps', u'leaders', u'cities', u'remained', u'news', u'relations', u'governor', u'civil', u'base', u'leadership', u'battle', u'officers', u'german', u'citizens', u'announced', u'russia', u'reports', u'germany', u'affairs', u'khrushchev', u'france', u'campaign', u'russian', u'leader', u'election', u'berlin', u'liberal', u'official', u'communism', u'china', u'headed', u'ordered', u'california', u'declared', u'senate', u'laos', u'victory', u'european', u'britain', u'chinese', u'congo', u'supported', u'republican', u'executive', u'troops', u'moscow', u'elections', u'royal', u'cuba', u'northern', u'determination', u'offices', u'pro', u'editorial', u'atlantic', u'representatives', u'peoples', u'poland', u'eastern', u'allies', u'allied', u'premier', u'nato']\n",
      "Cluster 42 : [u'shown', u'chapter', u'fig', u'billion', u'item', u'34', u'sept', u'00', u'respectively', u'cm', u'oct', u'32', u'aug', u'jan', u'estimates', u'sec', u'nov', u'mm', u'feb']\n",
      "Cluster 43 : [u'firmly']\n",
      "Cluster 44 : [u'spots', u'sheep', u'plug', u'yankee', u'tobacco']\n",
      "Cluster 45 : [u'jones']\n",
      "Cluster 46 : [u'back', u'long', u'men', u'still', u'right', u'came', u'house', u'place', u'around', u'small', u'went', u'left', u'got', u'hand', u'water', u'away', u'put', u'head', u'eyes', u'look', u'room', u'side', u'half', u'white', u'toward', u'face', u'big', u'looked', u'along', u'saw', u'light', u'open', u'turned', u'door', u'line', u'hands', u'body', u'car', u'across', u'together', u'air', u'boy', u'held', u'keep', u'behind', u'office', u'moment', u'run', u'street', u'turn', u'close', u'front', u'center', u'black', u'voice', u'top', u'red', u'road', u'stood', u'near', u'outside', u'fire', u'table', u'sound', u'looking', u'started', u'ground', u'dark', u'kept', u'taking', u'moved', u'heart', u'coming', u'inside', u'wall', u'followed', u'building', u'move', u'reached', u'hold', u'wide', u'river', u'passed', u'hall', u'police', u'suddenly', u'stand', u'hair', u'square', u'sat', u'lay', u'step', u'eye', u'hot', u'bed', u'ran', u'piece', u'blood', u'shot', u'horse', u'opened', u'sun', u'stopped', u'hotel', u'carried', u'running', u'hit', u'gun', u'deep', u'arms', u'straight', u'pool', u'main', u'heavy', u'window', u'steps', u'doctor', u'ball', u'moving', u'corner', u'slowly', u'forward', u'filled', u'poet', u'waiting', u'drive', u'closed', u'covered', u'arm', u'station', u'glass', u'distance', u'built', u'ahead', u'sea', u'reach', u'mouth', u'walk', u'teeth', u'trees', u'weight', u'standing', u'dropped', u'sides', u'enemy', u'entered', u'caught', u'cattle', u'bridge', u'sitting', u'thin', u'sign', u'bright', u'rose', u'key', u'fell', u'gray', u'bottom', u'kitchen', u'fast', u'clothes', u'quickly', u'mark', u'finished', u'carefully', u'bar', u'watch', u'spread', u'slightly', u'regular', u'rock', u'neck', u'knife', u'houses', u'fresh', u'train', u'apartment', u'watched', u'opposite', u'cross', u'edge', u'camp', u'picked', u'laid', u'watching', u'frame', u'buildings', u'bottle', u'turning', u'store', u'stayed', u'drawn', u'valley', u'box', u'thick', u'hill', u'dust', u'rain', u'horses', u'faces', u'dry', u'clean', u'broke', u'carrying', u'walls', u'waited', u'soft', u'lips', u'legs', u'block', u'traffic', u'faced', u'drew', u'dress', u'slow', u'holding', u'shoulder', u'fingers', u'chair', u'broken', u'yellow', u'wore', u'rifle', u'hung', u'desk', u'yards', u'towards', u'stone', u'sky', u'narrow', u'grew', u'empty', u'leg', u'tree', u'safe', u'loose', u'drove', u'cool', u'truck', u'spot', u'heavily', u'circle', u'wood', u'streets', u'stared', u'somewhere', u'roof', u'pale', u'onto', u'nose', u'marine', u'garden', u'tall', u'hole', u'wheel', u'draw', u'wagon', u'throat', u'rooms', u'hat', u'gold', u'entrance', u'driver', u'beneath', u'grass', u'driving', u'chest', u'breath', u'guard', u'bedroom', u'walking', u'seat', u'push', u'windows', u'wet', u'rear', u'pushed', u'silence', u'ice', u'wooden', u'via', u'tiny', u'pink', u'noticed', u'brilliant', u'rolled', u'reaching', u'swung', u'skin', u'flight', u'stairs', u'sheet', u'lights', u'jess', u'stepped', u'snake', u'shut', u'crossed', u'cloth', u'brush', u'yard', u'riding', u'flying', u'approached', u'shoes', u'path', u'dirt', u'darkness', u'colored', u'coat', u'climbed', u'stranger', u'porch', u'painted', u'lifted', u'golden', u'burned', u'restaurant', u'palace', u'doors', u'counter', u'tongue', u'rode', u'knee', u'curt', u'alex', u'sudden', u'sleeping', u'clouds', u'voices', u'leaned', u'gate', u'matsuo', u'lying', u'height', u'edges', u'shadow', u'stretched', u'mud', u'jumped', u'fence', u'bent', u'bench', u'jacket', u'bus', u'trail', u'split', u'parked', u'falling', u'burst', u'slipped', u'nick', u'locked', u'lighted', u'gathered', u'gardens', u'ceiling', u'blanket', u'barn', u'barely', u'bare', u'pressed', u'backed', u'alec', u'tilghman', u'shorts', u'shade', u'shirt', u'hanging', u'chin', u'flew', u'woods', u'staring', u'seated', u'blonde', u'beard', u'swing', u'pulling', u'glanced', u'flower', u'slid', u'nearest', u'hoag', u'heels', u'stiff', u'rourke', u'backward', u'skirt', u'sidewalk', u'shining', u'crack', u'shadows', u'rolling', u'peered', u'lobby', u'leaped', u'fountain', u'resting']\n",
      "Cluster 47 : [u'africa', u'asia']\n",
      "Cluster 48 : [u'shows']\n",
      "Cluster 49 : [u'blue', u'green', u'butter', u'creek', u'boots']\n",
      "Cluster 50 : [u'reducing']\n",
      "Cluster 51 : [u'marginal']\n",
      "Cluster 52 : [u'annual']\n",
      "Cluster 53 : [u'would', u'said', u'time', u'could', u'man', u'like', u'way', u'little', u'good', u'see', u'get', u'old', u'never', u'know', u'us', u'go', u'come', u'take', u'thought', u'say', u'let', u'something', u'think', u'enough', u'nothing', u'told', u'going', u'asked', u'knew', u'young', u'things', u'ever', u'felt', u'thing', u'want', u'done', u'anything', u'really', u'tell', u'sure', u'miss', u'mother', u'woman', u'heard', u'wife', u'girl', u'hard', u'father', u'wanted', u'feel', u'leave', u'soon', u'gone', u'call', u'everything', u'else', u'dead', u'cold', u'tried', u'care', u'getting', u'trying', u'talk', u'start', u'hear', u'husband', u'ready', u'happened', u'lot', u'anyone', u'yes', u'girls', u'bad', u'decided', u'try', u'remember', u'trouble', u'friend', u'maybe', u'met', u'wrong', u'chance', u'easy', u'ask', u'beautiful', u'oh', u'stop', u'lived', u'learned', u'ones', u'audience', u'stay', u'saying', u'wish', u'married', u'pretty', u'write', u'hardly', u'lord', u'jack', u'playing', u'hell', u'happy', u'bit', u'talking', u'someone', u'meant', u'fight', u'everyone', u'suppose', u'sir', u'wait', u'break', u'pass', u'names', u'captain', u'pain', u'ship', u'leaving', u'check', u'seeing', u'touch', u'spoke', u'sight', u'lady', u'fair', u'strange', u'remembered', u'impossible', u'drink', u'sam', u'forced', u'sweet', u'nice', u'memory', u'explained', u'coffee', u'nobody', u'mercer', u'dog', u'looks', u'telephone', u'quiet', u'die', u'boat', u'believed', u'weather', u'murder', u'fighting', u'everybody', u'killed', u'fit', u'send', u'kid', u'quick', u'wants', u'somehow', u'rome', u'dream', u'baby', u'smiled', u'beat', u'wind', u'warm', u'sit', u'dogs', u'buy', u'realize', u'pictures', u'minute', u'ought', u'jury', u'fellow', u'duty', u'tom', u'shop', u'phil', u'occurred', u'asking', u'aside', u'answered', u'sleep', u'jesus', u'gets', u'escape', u'wilson', u'supposed', u'somebody', u'snow', u'parker', u'join', u'kill', u'explain', u'beach', u'please', u'keeping', u'happen', u'fat', u'minister', u'careful', u'mine', u'imagine', u'eat', u'closer', u'refused', u'pull', u'negroes', u'joe', u'drop', u'struck', u'smile', u'perfect', u'notice', u'busy', u'birds', u'bear', u'wondered', u'wild', u'talked', u'surprised', u'pick', u'papa', u'liked', u'flowers', u'shook', u'replied', u'bought', u'alive', u'afraid', u'loved', u'guess', u'dozen', u'dear', u'wright', u'worry', u'wished', u'sounds', u'plenty', u'phone', u'hearst', u'alfred', u'wonderful', u'unable', u'thoughts', u'putting', u'guy', u'forget', u'cousin', u'breakfast', u'watson', u'warren', u'practically', u'plain', u'handle', u'flesh', u'crowd', u'telling', u'sick', u'pocket', u'knowing', u'grown', u'advice', u'surprise', u'papers', u'nodded', u'listen', u'laughed', u'hoped', u'burning', u'blind', u'worse', u'till', u'strike', u'silent', u'ring', u'proud', u'pair', u'moments', u'linda', u'drinking', u'conversation', u'charlie', u'roberts', u'ride', u'quietly', u'leaves', u'incident', u'holy', u'cry', u'count', u'cook', u'tired', u'sorry', u'occasionally', u'luck', u'honest', u'anger', u'wearing', u'singing', u'lucy', u'liquor', u'humor', u'fun', u'mama', u'handed', u'em', u'anyway', u'anne', u'adam', u'terrible', u'promised', u'promise', u'kate', u'hans', u'fired', u'calling', u'anybody', u'angry', u'younger', u'spirits', u'rachel', u'lovely', u'heaven', u'confused', u'brothers', u'whenever', u'violent', u'touched', u'throw', u'steady', u'smoke', u'sister', u'scotty', u'pike', u'henrietta', u'dancing', u'comfort', u'catch', u'sell', u'reply', u'lots', u'hate', u'guns', u'drunk', u'demanded', u'bag', u'stick', u'spoken', u'pleased', u'neighbors', u'funny', u'describe', u'bob', u'winston', u'tonight', u'thrown', u'shouted', u'missed', u'maris', u'magazine', u'joy', u'jim', u'happens', u'handsome', u'glance', u'forgotten', u'forever', u'focus', u'decide', u'castro', u'susan', u'stanley', u'robinson', u'mad', u'listening', u'instant', u'finish', u'clerk', u'briefly', u'bride', u'beer', u'bang', u'anywhere', u'temple', u'swimming', u'stomach', u'sharply', u'roy', u'paint', u'noise', u'glad', u'ears', u'dressed', u'supper', u'stronger', u'roll', u'plot', u'pat', u'pa', u'occupied', u'mood', u'ladies', u'innocent', u'hurt', u'fool', u'dirty', u'comfortable', u'chicken', u'bone', u'andy', u'visitors', u'thank', u'tears', u'smiling', u'rector', u'jane', u'hurry', u'harry', u'greg', u'crazy', u'cash', u'worried', u'waste', u'sounded', u'sad', u'meanwhile', u'lieutenant', u'jensen', u'gorton', u'fly', u'calm', u'tells', u'smell', u'seldom', u'reporters', u'naked', u'mistake', u'helva', u'dying', u'chandler', u'blame', u'bigger', u'nights', u'meal', u'lunch', u'eating', u'blow', u'bird', u'affair', u'wedding', u'tea', u'stuff', u'shouting', u'miriam', u'mickey', u'maid', u'kids', u'harold', u'gesture', u'felix', u'dave', u'damn', u'circles', u'checked', u'softly', u'shock', u'remark', u'ramey', u'precious', u'myra', u'letting', u'knocked', u'gently', u'eddie', u'devil', u'dan', u'sarah', u'players', u'pistol', u'penny', u'mighty', u'listened', u'johnny', u'hoping', u'grinned', u'ear', u'cried', u'clayton', u'brannon', u'asleep', u'wound', u'tight', u'stars', u'spencer', u'shoot', u'shayne', u'hal', u'gathering', u'fred', u'excuse', u'eileen', u'colony', u'card', u'cap', u'bullet', u'warmth', u'wally', u'upstairs', u'tale', u'sergeant', u'pot', u'paused', u'marshal', u'laughing', u'laugh', u'imagined', u'hated', u'happening', u'fled', u'dull', u'dawn', u'coolidge', u'christmas', u'barton', u'worn', u'wherever', u'tim', u'sweat', u'slept', u'seconds', u'rushed', u'permission', u'mirror', u'literally', u'julia', u'johnnie', u'honey', u'frightened', u'fist', u'eugene', u'ekstrohm', u'cady', u'breaking', u'bath', u'arlene', u'approaching', u'aged', u'unhappy', u'trembling', u'stuck', u'maggie', u'lonely', u'hungry', u'hang', u'fog', u'disturbed', u'coach', u'casey', u'cancer', u'blanche', u'billy', u'ah', u'yeah', u'wildly', u'tractor', u'torn', u'sheriff', u'scarcely', u'saddle', u'pile', u'patrol', u'pack', u'owen', u'killpath', u'hired', u'harm', u'gavin', u'faint', u'eyed', u'cigarette', u'bobby', u'belly', u'aboard', u'whisky', u'theresa', u'stupid', u'searching', u'pete', u'keith', u'jump', u'hurried', u'hide', u'freddy', u'burns', u'bobbie', u'ben', u'whispered', u'wake', u'stake', u'souls', u'sink', u'refrigerator', u'podger', u'midnight', u'killing', u'harder', u'gang', u'excited', u'elaine', u'dive', u'deck', u'boss', u'aunt', u'arrival', u'yelled', u'uneasy', u'startled', u'skyros', u'sighed', u'pitch', u'mess', u'maude', u'lean', u'laughter', u'joke', u'grip', u'freely', u'fault', u'dressing', u'desperately', u'dare', u'cromwell', u'cheek', u'cathy', u'bridget', u'bother', u'wondering', u'witnesses', u'ugly', u'stumbled', u'smart', u'slide', u'sisters', u'shame', u'scared', u'russ', u'reverend', u'rang', u'policeman', u'pitcher', u'neat', u'michelangelo', u'madden', u'lucky', u'lowered', u'loud', u'keeps', u'kay', u'joyce', u'hesitated', u'helpless', u'haney', u'dishes', u'buck', u'brushed', u'awake', u'ada', u'abel', u'worries', u'whip', u'weary', u'tent', u'rush', u'rob', u'reporter', u'pipe', u'pamela', u'okay', u'nadine', u'laura', u'kitti', u'impulse', u'hidden', u'happily', u'guys', u'figured', u'dolores', u'doc', u'cafe', u'borden', u'bet', u'anyhow', u'andrei', u'wiped', u'tommy', u'snapped', u'smelled', u'shocked', u'salem', u'rider']\n",
      "Cluster 54 : [u'foot', u'inch', u'inches']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 55 : [u'radiation', u'observed', u'moon', u'intensity', u'measurements', u'wave', u'observations', u'thermal', u'emission', u'temperatures', u'planets', u'planet', u'planetary', u'solar']\n",
      "Cluster 56 : [u'systems']\n",
      "Cluster 57 : [u'operations']\n",
      "Cluster 58 : [u'analysis']\n",
      "Cluster 59 : [u'james']\n",
      "Cluster 60 : [u'normal']\n",
      "Cluster 61 : [u'firm']\n",
      "Cluster 62 : [u'generally']\n",
      "Cluster 63 : [u'chief']\n",
      "Cluster 64 : [u'depends']\n",
      "Cluster 65 : [u'1', u'2', u'3', u'per', u'4', u'5', u'10', u'total', u'million', u'6', u'8', u'1960', u'7', u'30', u'cent', u'15', u'20', u'12', u'50', u'average', u'25', u'100', u'0', u'1959', u'9', u'11', u'degrees', u'14', u'16', u'maximum', u'500', u'24', u'april', u'40', u'60', u'compared', u'approximately', u'18', u'estimated', u'22', u'13', u'21', u'200', u'23', u'percent', u'17', u'january', u'equivalent', u'diameter', u'february', u'shares', u'pounds', u'70', u'26', u'acres', u'19', u'75', u'36', u'ratio', u'net', u'27', u'80', u'mg', u'cents', u'29', u'28', u'lb', u'45', u'tons', u'90', u'85']\n",
      "Cluster 66 : [u'items']\n",
      "Cluster 67 : [u'located']\n",
      "Cluster 68 : [u'price']\n",
      "Cluster 69 : [u'mr', u'john', u'secretary', u'brown', u'son', u'letter', u'george', u'de', u'thomas', u'director', u'charles', u'mary', u'brother', u'named', u'robert', u'judge', u'morgan', u'daughter', u'richard', u'louis', u'frank', u'attorney', u'honor', u'v', u'smith', u'lewis', u'uncle', u'eisenhower', u'martin', u'vice', u'professor', u'joseph', u'david', u'author', u'speaker', u'remarks', u'k', u'arthur', u'mention', u'mayor', u'edward', u'representative', u'peter', u'lawyer', u'johnson', u'carl', u'admitted', u'thompson', u'walter', u'paul', u'dean', u'senator', u'lawrence', u'colonel', u'jackson', u'lee', u'rev', u'clark', u'baker', u'attended', u'assistant', u'williams', u'samuel', u'morse', u'funeral', u'howard', u'albert', u'van', u'jefferson', u'hughes', u'harris', u'sen', u'davis', u'jean', u'appointment', u'mitchell', u'victor', u'moore', u'houston', u'ambassador', u'vernon', u'ford', u'taylor', u'gen', u'douglas', u'ralph', u'morris', u'francis', u'allen', u'meredith', u'gov', u'donald', u'austin', u'superintendent', u'stephen']\n",
      "Cluster 70 : [u'chairman']\n",
      "Cluster 71 : [u'one', u'letters', u'older', u'famous', u'team', u'rich', u'wine', u'song', u'failed', u'discovered', u'beauty', u'animal', u'revolution', u'independence', u'forth', u'settled', u'fashion', u'ends', u'relief', u'odd', u'ancient', u'wonder', u'stands', u'plays', u'passing', u'object', u'imagination', u'dominant', u'greek', u'bodies', u'unusual', u'pleasure', u'la', u'informed', u'grow', u'argument', u'save', u'orchestra', u'motion', u'friendly', u'feelings', u'stories', u'repeated', u'novel', u'confidence', u'taste', u'songs', u'roman', u'painting', u'lie', u'soldiers', u'occasion', u'nineteenth', u'lose', u'indian', u'avoid', u'sought', u'permitted', u'notes', u'hero', u'generation', u'enjoyed', u'combination', u'apart', u'spite', u'pure', u'pre', u'minds', u'jews', u'win', u'theme', u'symbol', u'properly', u'inner', u'huge', u'device', u'detective', u'description', u'demands', u'childhood', u'suggest', u'sin', u'mantle', u'finding', u'female', u'bound', u'version', u'japanese', u'experienced', u'disease', u'depth', u'contrary', u'bitter', u'eventually', u'views', u'television', u'supreme', u'soul', u'gradually', u'easier', u'begun', u'agree', u'ages', u'taught', u'latin', u'convinced', u'choose', u'violence', u'tragedy', u'tendency', u'suit', u'personality', u'passage', u'liberty', u'constitution', u'target', u'rayburn', u'precisely', u'poem', u'opera', u'medium', u'instrument', u'fought', u'destroy', u'concrete', u'centuries', u'cast', u'brain', u'valuable', u'unknown', u'testimony', u'surely', u'rough', u'remarkable', u'reader', u'numerous', u'movements', u'mere', u'italian', u'fears', u'everywhere', u'discovery', u'deny', u'denied', u'creation', u'congregation', u'weakness', u'superior', u'pride', u'identified', u'doctrine', u'difficulties', u'dangerous', u'curious', u'adams', u'warning', u'slavery', u'qualities', u'prime', u'offers', u'impression', u'feels', u'expensive', u'core', u'ballet', u'arranged', u'agent', u'wisdom', u'visual', u'suffering', u'slaves', u'pace', u'mostly', u'helping', u'errors', u'enjoy', u'drama', u'confusion', u'answers', u'accomplished', u'warfare', u'separated', u'saved', u'republic', u'raw', u'prison', u'pope', u'personally', u'mankind', u'insisted', u'explanation', u'emotions', u'believes', u'anxiety', u'wise', u'weapon', u'unlike', u'threat', u'sufficiently', u'suffered', u'skill', u'replaced', u'reflected', u'reactions', u'queen', u'match', u'increasingly', u'host', u'holds', u'express', u'distinguished', u'criticism', u'comment', u'writes', u'whereas', u'virtually', u'teach', u'shakespeare', u'saxon', u'sake', u'remaining', u'rarely', u'rare', u'musicians', u'limits', u'involves', u'images', u'folk', u'favorite', u'distinction', u'display', u'destruction', u'constantly', u'communists', u'assumption', u'thoroughly', u'talent', u'studying', u'shared', u'severe', u'prominent', u'notion', u'neutral', u'necessity', u'mystery', u'languages', u'intense', u'grace', u'existed', u'exception', u'discover', u'brings', u'afford', u'striking', u'species', u'spanish', u'revealed', u'relationships', u'recall', u'prokofieff', u'primitive', u'newspapers', u'holmes', u'guest', u'gained', u'furniture', u'destroyed', u'deeply', u'consciousness', u'comedy', u'attractive', u'assured', u'turns', u'tough', u'simultaneously', u'sacred', u'pleasant', u'piano', u'magic', u'japan', u'inevitably', u'fail', u'experts', u'copy', u'catholics', u'bringing', u'attempts', u'arrangements', u'accounts', u'tremendous', u'thanks', u'swift', u'strongly', u'release', u'profession', u'occasional', u'moves', u'mixed', u'male', u'keys', u'illusion', u'feature', u'examine', u'effectively', u'distant', u'devices', u'delivered', u'definite', u'deeper', u'conclusions', u'chose', u'camera', u'belong', u'aim', u'admit', u'accompanied', u'twentieth', u'sympathy', u'symbols', u'symbolic', u'swept', u'sing', u'sherman', u'sees', u'scientists', u'satisfied', u'roles', u'pip', u'patients', u'paintings', u'owner', u'normally', u'managed', u'license', u'intention', u'fellowship', u'error', u'desirable', u'coal', u'classic', u'characters', u'challenge', u'beings', u'automatically', u'affected', u'worst', u'wear', u'tied', u'sympathetic', u'sciences', u'routine', u'preserve', u'prepare', u'phrase', u'performed', u'palfrey', u'newly', u'nationalism', u'kohnstamm', u'fruit', u'examination', u'emotion', u'divine', u'disappeared', u'dignity', u'crime', u'corn', u'context', u'conservative', u'canada', u'badly', u'accurate', u'abstract', u'worker', u'visible', u'symphony', u'suggestion', u'shortly', u'sending', u'realistic', u'plato', u'openly', u'mechanical', u'jew', u'indians', u'historian', u'harmony', u'gift', u'frequent', u'fallen', u'facing', u'exposed', u'dispute', u'decades', u'banion', u'artistic', u'arrangement', u'alternative', u'unfortunately', u'tragic', u'survive', u'suffer', u'succeeded', u'substance', u'steele', u'slave', u'sharpe', u'seventh', u'satisfaction', u'roosevelt', u'romantic', u'presents', u'presentation', u'precise', u'performances', u'patchen', u'missing', u'massive', u'leads', u'interview', u'inevitable', u'healthy', u'guilt', u'grave', u'fate', u'entry', u'demonstrated', u'composer', u'classical', u'awareness', u'attempted', u'alert', u'advised', u'accident', u'woodruff', u'weak', u'wagner', u'theatre', u'strain', u'salvation', u'sacrifice', u'russians', u'remote', u'remarked', u'reflection', u'poets', u'partner', u'missiles', u'lacking', u'farmer', u'excitement', u'era', u'elaborate', u'discussions', u'discipline', u'defeat', u'dancers', u'courage', u'confronted', u'concluded', u'conception', u'citizen', u'victim', u'troubled', u'successfully', u'studio', u'structures', u'stable', u'silver', u'ruth', u'root', u'recording', u'radical', u'protected', u'prize', u'perfectly', u'pages', u'outer', u'muscles', u'movies', u'movie', u'mature', u'invariably', u'helps', u'gay', u'franklin', u'films', u'false', u'fallout', u'extraordinary', u'doctors', u'dancer', u'christianity', u'bearing', u'ann', u'widespread', u'virtue', u'unconscious', u'suspect', u'surprising', u'sovereign', u'semi', u'sang', u'reveal', u'republicans', u'purely', u'propaganda', u'nowhere', u'mothers', u'impressed', u'hypothalamic', u'frontier', u'fromm', u'faulkner', u'ex', u'encountered', u'dreams', u'deliberately', u'dates', u'crucial', u'creating', u'controls', u'consequence', u'conceived', u'complicated', u'altogether', u'absolute', u'vigorous', u'upward', u'trevelyan', u'threatened', u'testament', u'tasks', u'sons', u'shapes', u'scenes', u'reminded', u'questioned', u'proportion', u'perception', u'morality', u'merit', u'lesson', u'ignored', u'helpful', u'guilty', u'furnish', u'frozen', u'exciting', u'eternal', u'encouraged', u'educated', u'ecumenical', u'drivers', u'divorce', u'delight', u'constitute', u'consistent', u'conductor', u'commonly', u'cloud', u'argued', u'argue', u'applying', u'anxious', u'anglo', u'worthy', u'witness', u'ward', u'verse', u'suite', u'sovereignty', u'southeast', u'saving', u'responses', u'reputation', u'promptly', u'peculiar', u'passion', u'offering', u'observation', u'miller', u'mechanism', u'marks', u'losing', u'liberals', u'judgments', u'innocence', u'humanity', u'gentleman', u'formerly', u'folklore', u'exact', u'enthusiasm', u'encounter', u'eighteenth', u'discuss', u'destructive', u'designs', u'demonstrate', u'delicate', u'convention', u'committed', u'clarity', u'certainty', u'broadway', u'aristotle', u'arise', u'african', u'utterly', u'traders', u'theological', u'suspicion', u'sitter', u'scholars', u'reserved', u'register', u'refer', u'publicly', u'publicity', u'profound', u'prevented', u'prefer', u'placing', u'philosophical', u'outdoor', u'oral', u'mutual', u'motel', u'marriages', u'magnificent', u'kingdom', u'inspired', u'insist', u'injury', u'horizon', u'handled', u'germans', u'gentle', u'friendship', u'fortune', u'enemies', u'eager', u'di', u'critics', u'concepts', u'acquire', u'absolutely', u'zen', u'widow', u'wholly', u'wars', u'treat', u'threatening', u'terror', u'teen', u'southerners', u'sophisticated', u'snakes', u'ruling', u'repeat', u'regiment', u'recalled', u'preferred', u'pointing', u'phenomena', u'perspective', u'painter', u'overcome', u'nixon', u'mysterious', u'masters', u'lively', u'legend', u'killer', u'justify', u'intelligent', u'instruments', u'inherent', u'identify', u'hunter', u'exposure', u'disaster', u'desperate', u'delightful', u'critic', u'controversy', u'contest', u'climate', u'charm', u'cavalry', u'bears', u'barco', u'appreciate', u'aesthetic', u'actor', u'vivid', u'viewed', u'vague', u'tooth', u'succession', u'subtle', u'shelters', u'sandburg', u'ritual', u'responsibilities', u'reflect', u'recommend', u'rational', u'racial', u'quiney', u'proceeded', u'pressing', u'pittsburgh', u'partially', u'parade', u'painful', u'occupation', u'observe', u'musician', u'mason', u'magazines', u'krim', u'impressions', u'imitation', u'grateful', u'generous', u'exhibit', u'evidently', u'elementary', u'dilemma', u'demonstration', u'delayed', u'craft', u'continuity', u'continually', u'contacts', u'constitutional', u'circuit', u'cheap', u'attracted', u'attacked', u'attached', u'approaches', u'amateur', u'accused', u'abandoned', u'wines', u'vienna', u'utopia', u'undoubtedly', u'tribute', u'tended', u'subjected', u'shell', u'rhythm', u'relieved', u'realization', u'realism', u'qualified', u'puerto', u'promising', u'norms', u'narrative', u'mexican', u'meaningful', u'mathematical', u'managers', u'loving', u'lift', u'lawyers', u'justified', u'interpreted', u'imposed', u'happiness', u'functional', u'forgive', u'expressing', u'explicit', u'exclusively', u'enthusiastic', u'entering', u'empirical', u'diffusion', u'desires', u'depression', u'democracy', u'declaration', u'cycle', u'criminal', u'clubs', u'cited', u'charming', u'chances', u'breed', u'brave', u'biggest', u'balanced', u'authors', u'astronomy', u'architect', u'angels', u'accurately', u'westminster', u'verbal', u'variations', u'unexpected', u'ultimately', u'trace', u'thinks', u'suited', u'suggestions', u'strategic', u'sponsor', u'sixties', u'settle', u'sentiment', u'semitism', u'select', u'resumed', u'relevant', u'registered', u'regime', u'reflects', u'protest', u'posts', u'philip', u'partisan', u'originally', u'noble', u'ninth', u'moderate', u'milton', u'lighting', u'incredible', u'hearts', u'habit', u'giant', u'genius', u'generations', u'expectations', u'exercises', u'enable', u'dominated', u'dances', u'curiosity', u'convenient', u'controlling', u'compete', u'colleagues', u'client', u'cleared', u'boating', u'blues', u'beliefs', u'bases', u'attempting', u'assumptions', u'anticipated', u'ambiguous', u'adults', u'wit', u'wealth', u'unfortunate', u'uncertain', u'troubles', u'triumph', u'treaty', u'totally', u'targets', u'surrender', u'substitute', u'steadily', u'spectacular', u'servants', u'savage', u'rousseau', u'revolutionary', u'resist', u'relatives', u'regularly', u'poverty', u'possessed', u'physiological', u'physics', u'patience', u'occurring', u'occasions', u'novels', u'nerves', u'motive', u'minded', u'merchants', u'meanings', u'makers', u'loyalty', u'intentions', u'insight', u'implications', u'grows', u'glory', u'fortunate', u'flood', u'expects', u'exhibition', u'emperor', u'eliminated', u'earliest', u'doubtful', u'destiny', u'delay', u'dealt', u'conspiracy', u'congressman', u'complained', u'chiefly', u'chart', u'cease', u'casual', u'binding', u'belongs', u'attacks', u'appreciation', u'altered', u'alienation', u'1948', u'wives', u'wing', u'winds', u'utopian', u'urgent', u'urge', u'unlikely', u'trends', u'traditions', u'thorough', u'therapist', u'telegraph', u'tales', u'suspected', u'surrounded', u'speeches', u'socialist', u'slim', u'rico', u'reveals', u'respond', u'respectable', u'resolved', u'races', u'questioning', u'punishment', u'possession', u'pioneer', u'persuaded', u'pause', u'obliged', u'murderer', u'mode', u'merger', u'melody', u'malraux', u'luxury', u'letch', u'knight', u'katanga', u'johnston', u'intimate', u'inspection', u'inclined', u'ideological', u'heroic', u'heritage', u'helion', u'hatred', u'hammarskjold', u'habits', u'governing', u'gentlemen', u'enjoyment', u'emphasize', u'drawings', u'displays', u'displayed', u'diet', u'despair', u'defend', u'dedication', u'cuban', u'creatures', u'cope', u'conditioned', u'complement', u'competent', u'colorful', u'colonial', u'circular', u'christians', u'bullets', u'bold', u'belgians', u'behalf', u'associate', u'angel', u'addresses', u'accepting', u'acceptable', u'welch', u'warrant', u'vs', u'underlying', u'tones', u'thereafter', u'theories', u'theology', u'temporarily', u'tactics', u'styles', u'struggling', u'strongest', u'string', u'strikes', u'stalin', u'splendid', u'spell', u'solve', u'socialism', u'sloan', u'secrets', u'responded', u'renaissance', u'reactionary', u'ranks', u'quarrel', u'puts', u'pupil', u'promises', u'preserved', u'physically', u'pen', u'passages', u'overwhelming', u'orderly', u'oersted', u'oedipus', u'observers', u'notte', u'notable', u'newer', u'negotiations', u'motives', u'mercy', u'merchant', u'mathematics', u'manage', u'lover', u'listeners', u'lesser', u'landscape', u'jungle', u'invitation', u'invention', u'injured', u'indication', u'illness', u'hotels', u'historians', u'hetman', u'guitar', u'guided', u'griffith', u'greeted', u'founded', u'fortunately', u'fitting', u'fascinating', u'fantastic', u'explains', u'enforced', u'endless', u'emphasized', u'eichmann', u'distinctive', u'decent', u'deadly', u'copernicus', u'convictions', u'converted', u'confirmed', u'compromise', u'clothing', u'byron', u'berger', u'basically', u'baptist', u'authentic', u'attain', u'aroused', u'anticipation', u'amazing', u'alike', u'accordance', u'vincent', u'victims', u'tensions', u'tangible', u'sung', u'stresses', u'startling', u'solved', u'sober', u'shelley', u'servant', u'rugged', u'rod', u'ridiculous', u'rid', u'reverse', u'respective', u'relax', u'realtors', u'realm', u'puzzled', u'produces']\n",
      "Cluster 72 : [u'sharp']\n",
      "Cluster 73 : [u'reasonable']\n",
      "Cluster 74 : [u'association']\n",
      "Cluster 75 : [u'negative']\n",
      "Cluster 76 : [u'first', u'year', u'day', u'last', u'home', u'night', u'end', u'next', u'later', u'days', u'early', u'began', u'week', u'times', u'past', u'period', u'town', u'morning', u'play', u'months', u'wrote', u'late', u'hour', u'hours', u'st', u'rest', u'meeting', u'fall', u'month', u'earlier', u'boys', u'weeks', u'sent', u'summer', u'nearly', u'1961', u'game', u'appeared', u'evening', u'spring', u'series', u'worked', u'length', u'daily', u'march', u'returned', u'season', u'cars', u'hospital', u'visit', u'afternoon', u'sunday', u'date', u'spent', u'round', u'june', u'worth', u'1958', u'london', u'dinner', u'yesterday', u'fourth', u'previous', u'opening', u'died', u'winter', u'trip', u'editor', u'session', u'league', u'mile', u'twice', u'planned', u'november', u'page', u'saturday', u'monday', u'paris', u'completed', u'starting', u'july', u'clock', u'tomorrow', u'newspaper', u'guests', u'december', u'boston', u'palmer', u'friday', u'arrived', u'ended', u'baseball', u'september', u'tuesday', u'player', u'runs', u'attend', u'spend', u'august', u'fifth', u'prior', u'vacation', u'journal', u'decade', u'tv', u'quarter', u'31', u'concert', u'mid', u'scheduled', u'golf', u'wednesday', u'italy', u'thursday', u'sentence', u'1962', u'fourteen', u'ending', u'journey', u'sessions', u'festival', u'seventeen', u'1950', u'storm', u'concerts', u'noon', u'luncheon', u'arrive', u'autumn', u'anniversary', u'tournament']\n",
      "Cluster 77 : [u'chicago']\n",
      "Cluster 78 : [u'electric']\n",
      "Cluster 79 : [u'rate', u'increase', u'increases', u'wage']\n",
      "Cluster 80 : [u'games']\n",
      "Cluster 81 : [u'opposition']\n",
      "Cluster 82 : [u'membership']\n",
      "Cluster 83 : [u'tests']\n",
      "Cluster 84 : [u'completion', u'onset']\n",
      "Cluster 85 : [u'mrs', u'c', u'b', u'p', u'e', u'dr', u'n', u'william', u'f', u'r', u'j', u'l', u'h', u'w', u'g', u'henry', u'jr']\n",
      "Cluster 86 : [u'flux']\n",
      "Cluster 87 : [u'efficiency']\n",
      "Cluster 88 : [u'lower']\n",
      "Cluster 89 : [u'employed']\n",
      "Cluster 90 : [u'national', u'members', u'local', u'board', u'committee', u'medical', u'international', u'member', u'organization', u'council', u'conference']\n",
      "Cluster 91 : [u'may', u'also', u'used', u'use', u'high', u'however', u'general', u'number', u'public', u'less', u'set', u'system', u'business', u'program', u'form', u'given', u'large', u'possible', u'within', u'interest', u'development', u'area', u'service', u'different', u'means', u'field', u'information', u'full', u'major', u'special', u'federal', u'cost', u'economic', u'problems', u'study', u'job', u'available', u'result', u'change', u'level', u'areas', u'force', u'control', u'type', u'land', u'surface', u'necessary', u'tax', u'following', u'military', u'low', u'provide', u'value', u'cut', u'range', u'various', u'single', u'lines', u'section', u'pressure', u'stage', u'values', u'needed', u'industry', u'greater', u'expected', u'space', u'basis', u'required', u'complete', u'conditions', u'support', u'material', u'costs', u'forces', u'developed', u'research', u'pay', u'amount', u'added', u'including', u'basic', u'defense', u'equipment', u'higher', u'terms', u'market', u'labor', u'similar', u'property', u'growth', u'food', u'stock', u'results', u'production', u'cases', u'aid', u'increased', u'using', u'effort', u'size', u'temperature', u'methods', u'method', u'due', u'addition', u'record', u'directly', u'according', u'programs', u'physical', u'volume', u'sales', u'list', u'direct', u'provided', u'changes', u'plant', u'planning', u'respect', u'effective', u'treatment', u'somewhat', u'based', u'charge', u'reaction', u'radio', u'operation', u'numbers', u'latter', u'manner', u'larger', u'test', u'technical', u'determined', u'described', u'additional', u'quality', u'design', u'types', u'specific', u'pattern', u'activity', u'obtained', u'growing', u'activities', u'parts', u'standard', u'mass', u'include', u'products', u'lack', u'extent', u'designed', u'serve', u'machine', u'limited', u'indicated', u'determine', u'continue', u'factors', u'current', u'applied', u'agreement', u'unit', u'prepared', u'management', u'energy', u'studies', u'rise', u'rates', u'original', u'supply', u'related', u'demand', u'oil', u'heat', u'gas', u'facilities', u'techniques', u'materials', u'actual', u'included', u'active', u'proper', u'construction', u'source', u'product', u'structure', u'objective', u'complex', u'speed', u'records', u'purposes', u'produced', u'measure', u'build', u'region', u'equal', u'add', u'sources', u'scientific', u'operating', u'cover', u'carry', u'units', u'stations', u'require', u'loss', u'distribution', u'variety', u'relatively', u'capacity', u'requirements', u'produce', u'provides', u'essential', u'indicate', u'smaller', u'separate', u'reduced', u'economy', u'differences', u'atmosphere', u'plus', u'gain', u'permit', u'increasing', u'selected', u'ordinary', u'phase', u'resources', u'constant', u'allow', u'scale', u'factor', u'exchange', u'protection', u'levels', u'flow', u'reference', u'minimum', u'initial', u'appropriate', u'application', u'teaching', u'potential', u'impact', u'sufficient', u'substantial', u'adequate', u'location', u'relation', u'maintenance', u'goal', u'reduce', u'chemical', u'procedures', u'prices', u'continuing', u'solution', u'maintain', u'existing', u'uses', u'providing', u'useful', u'processes', u'represented', u'preparation', u'improved', u'detailed', u'conventional', u'foods', u'achieve', u'desired', u'electrical', u'comparison', u'cooling', u'continuous', u'storage', u'resulting', u'occur', u'marketing', u'processing', u'possibilities', u'comparable', u'satisfactory', u'improve', u'determining', u'evaluation']\n",
      "Cluster 92 : [u'measures']\n",
      "Cluster 93 : [u'data', u'resolution', u'essentially', u'edition', u'crystal']\n",
      "Cluster 94 : [u'derived']\n",
      "Cluster 95 : [u'figures', u'pieces', u'upper', u'solid', u'hanover', u'model', u'flat', u'shelter', u'cutting', u'signs', u'score', u'signal', u'metal', u'experiments', u'portion', u'remove', u'motor', u'animals', u'soil', u'extra', u'iron', u'fill', u'band', u'colors', u'artery', u'waves', u'uniform', u'sports', u'screen', u'salt', u'milk', u'grade', u'driven', u'steel', u'sold', u'resistance', u'sum', u'models', u'meat', u'fed', u'chamber', u'wire', u'panels', u'mounted', u'losses', u'load', u'finger', u'washing', u'switch', u'excess', u'cup', u'charges', u'volumes', u'tables', u'drawing', u'tool', u'seed', u'consists', u'tissue', u'thyroid', u'smooth', u'seeds', u'regions', u'drug', u'bread', u'bars', u'automatic', u'parallel', u'indicates', u'holes', u'cotton', u'composed', u'atoms', u'wash', u'transfer', u'track', u'maturity', u'foam', u'estimate', u'equipped', u'cleaning', u'atom', u'washed', u'reception', u'pressures', u'organic', u'constructed', u'bomb', u'35', u'resulted', u'questionnaire', u'illustrated', u'filling', u'crew', u'bombs', u'blocks', u'schedule', u'printed', u'columns', u'cards', u'accuracy', u'yield', u'row', u'pound', u'ocean', u'lumber', u'installed', u'fish', u'eggs', u'bull', u'beef', u'400', u'sugar', u'ranging', u'measurement', u'lists', u'lane', u'drill', u'absent', u'rice', u'quantity', u'pulmonary', u'plastic', u'panel', u'measuring', u'horn', u'fewer', u'carbon', u'branch', u'winning', u'variation', u'super', u'stained', u'skywave', u'quarters', u'lightly', u'juniors', u'jet', u'freight', u'dried', u'depending', u'cure', u'cow', u'covers', u'covering', u'cooking', u'1952', u'tossed', u'stem', u'starts', u'saline', u'parking', u'islands', u'grain', u'frequencies', u'drying', u'buying', u'belt', u'basement', u'suspended', u'strip', u'replace', u'preceding', u'prayer', u'poured', u'mixture', u'excessive', u'et', u'directions', u'cuts', u'concentrated', u'closing', u'barrel', u'800', u'37', u'subsequent', u'signals', u'shots', u'sewage', u'santa', u'sand', u'samples', u'prevention', u'planes', u'glasses', u'fabrics', u'examined', u'eighth', u'dining', u'dimensional', u'combat', u'bronchial', u'apparatus', u'1956', u'trading', u'surfaces', u'roots', u'powder', u'mines', u'holder', u'al', u'wings', u'surrounding', u'shaped', u'scattered', u'released', u'radar', u'manufacturer', u'leather', u'landing', u'gear', u'emerged', u'consisting', u'600', u'urethane', u'tail', u'switches', u'suits', u'stretch', u'rigid', u'meals', u'lock', u'hr', u'frames', u'fogg', u'firing', u'electricity', u'detergent', u'cellar', u'auto', u'vein', u'thrust', u'stern', u'roughly', u'rocks', u'reaches', u'pupils', u'protein', u'intervals', u'heating', u'frequency', u'flexible', u'disk', u'composition', u'cocktail', u'categories', u'cat', u'tubes', u'tie', u'sphere', u'specimen', u'seized', u'prisoners', u'plate', u'peas', u'palm', u'nervous', u'manufacturing', u'loaded', u'heights', u'consisted', u'bend', u'accomplish', u'250', u'wounded', u'tip', u'teams', u'substances', u'spare', u'soap', u'rifles', u'registration', u'plates', u'packed', u'lengths', u'insects', u'illustration', u'herd', u'grades', u'gin', u'giants', u'furnished', u'fraction', u'fibers', u'economical', u'drinks', u'continuously', u'channels', u'chairs', u'cabin', u'bowl', u'bombers', u'65', u'warned', u'tsunami', u'trucks', u'trap', u'tire', u'sticks', u'springs', u'shu', u'screw', u'sampling', u'racing', u'preparing', u'passenger', u'package', u'melting', u'mate', u'lo', u'lined', u'hits', u'hen', u'hay', u'garage', u'formulas', u'foams', u'fluid', u'flash', u'dikkat', u'conversion', u'classification', u'cholesterol', u'adding', u'52', u'wheels', u'trim', u'theoretical', u'summary', u'sub', u'stroke', u'span', u'rent', u'passengers', u'nuts', u'nude', u'mustard', u'milligrams', u'mars', u'marble', u'loop', u'ladder', u'indirect', u'ham', u'garibaldi', u'forming', u'fans', u'envelope', u'desert', u'definitely', u'collar', u'characterized', u'breathing', u'bod', u'beam', u'1951', u'votes', u'suitcase', u'submarines', u'slender', u'sauce', u'sailing', u'rises', u'purchased', u'nest', u'lungs', u'illuminated', u'haired', u'grains', u'gains', u'fractions', u'foil', u'fitted', u'fires', u'fee', u'el', u'earned', u'dick', u'dated', u'dairy', u'cylinder', u'crown', u'crop', u'cream', u'crash', u'brass', u'bones', u'battery', u'42', u'1949', u'wildlife', u'underground', u'twisted', u'tsh', u'toes', u'toast', u'tetrachloride', u'systematic', u'steam', u'slip', u'sketches', u'situated', u'shorter', u'shipping', u'sera', u'salesmen', u'russell', u'reactivity']\n",
      "Cluster 96 : [u'clay', u'mold', u'plaster']\n",
      "Cluster 97 : [u'population']\n",
      "Cluster 98 : [u'island', u'paid', u'bill', u'term', u'farm', u'fiscal', u'responsibility', u'income', u'division', u'commission', u'rhode', u'health', u'share', u'authority', u'funds', u'corporation', u'project', u'balance', u'principal', u'workers', u'security', u'published', u'capital', u'assistance', u'companies', u'financial', u'classes', u'proposed', u'collection', u'prevent', u'junior', u'formed', u'vocational', u'rights', u'title', u'receive', u'personnel', u'vote', u'interior', u'claims', u'fund', u'aircraft', u'legal', u'fields', u'educational', u'du', u'directed', u'achievement', u'projects', u'policies', u'electronic', u'article', u'gross', u'employees', u'closely', u'site', u'jobs', u'forest', u'armed', u'primarily', u'credit', u'internal', u'domestic', u'competition', u'budget', u'benefit', u'assignment', u'travel', u'trained', u'rising', u'railroad', u'library', u'homes', u'agencies', u'weapons', u'organizations', u'governments', u'courses', u'contract', u'task', u'machinery', u'india', u'housing', u'roads', u'plants', u'minor', u'goods', u'establish', u'advance', u'orders', u'limit', u'emphasis', u'commerce', u'charged', u'review', u'granted', u'entitled', u'conduct', u'apply', u'agency', u'welfare', u'publication', u'machines', u'firms', u'extended', u'divided', u'conducted', u'rural', u'positions', u'owned', u'interpretation', u'vehicles', u'practices', u'operate', u'motors', u'massachusetts', u'largest', u'advanced', u'administrative', u'widely', u'raise', u'introduced', u'foundation', u'establishment', u'developing', u'correct', u'centers', u'abroad', u'opportunities', u'missile', u'investigation', u'institute', u'grant', u'estate', u'devoted', u'boats', u'approval', u'advertising', u'towns', u'pont', u'navy', u'manufacturers', u'extremely', u'expense', u'courts', u'automobile', u'shooting', u'request', u'millions', u'impressive', u'hopes', u'safety', u'maintained', u'intelligence', u'engineering', u'employment', u'trend', u'supplies', u'purchase', u'provision', u'periods', u'mail', u'loan', u'legislation', u'interference', u'insurance', u'expenses', u'engaged', u'considering', u'bonds', u'boards', u'bills', u'appointed', u'recommended', u'pilot', u'percentage', u'encourage', u'award', u'atomic', u'1954', u'taxes', u'residential', u'referred', u'rapid', u'expenditures', u'elsewhere', u'depend', u'controlled', u'adopted', u'sale', u'raising', u'muscle', u'external', u'extensive', u'developments', u'contribute', u'collected', u'burden', u'amounts', u'urban', u'transportation', u'tend', u'technology', u'salary', u'recreation', u'mental', u'investment', u'institution', u'concerns', u'bureau', u'wages', u'varying', u'varied', u'regional', u'legislative', u'handling', u'engineer', u'ease', u'treasury', u'spending', u'selection', u'proposal', u'participation', u'opposed', u'legislature', u'desegregation', u'dependent', u'customers', u'continues', u'connected', u'communities', u'code', u'regarding', u'presumably', u'permanent', u'owners', u'laboratory', u'introduction', u'improvement', u'highway', u'guidance', u'goals', u'contributed', u'approved', u'survey', u'provisions', u'objectives', u'joint', u'furthermore', u'emergency', u'cooperation', u'consumer', u'colleges', u'candidates', u'authorities', u'allotment', u'agents', u'acts', u'tested', u'specifically', u'serving', u'represent', u'instructions', u'districts', u'banks', u'agricultural', u'skills', u'signed', u'serves', u'reserve', u'producing', u'patent', u'outstanding', u'hunting', u'guide', u'formation', u'farmers', u'extension', u'enormous', u'contribution', u'authorized', u'assure', u'supplied', u'substantially', u'senior', u'scholarship', u'profit', u'outlook', u'kinds', u'industries', u'engineers', u'vehicle', u'stores', u'sponsored', u'revenue', u'retired', u'presently', u'latest', u'counties', u'consequences', u'claimed', u'affect', u'adjustment', u'adjusted', u'vary', u'tools', u'thereby', u'suitable', u'strictly', u'selling', u'returns', u'receiving', u'reasonably', u'protect', u'presidential', u'metropolitan', u'matching', u'logical', u'findings', u'financing', u'duties', u'currently', u'textile', u'rejected', u'pathology', u'naval', u'involving', u'household', u'favorable', u'electronics', u'efficient', u'dealers', u'damage', u'collective', u'builder', u'branches', u'benefits', u'universities', u'transition', u'testing', u'survival', u'skilled', u'ruled', u'promote', u'medicine', u'loans', u'initiative', u'inadequate', u'effectiveness', u'debate', u'competitive', u'commander', u'utility', u'secondary', u'plastics', u'petitioner', u'markets', u'mainly', u'involve', u'import', u'finance', u'extend', u'expanding', u'enterprise', u'decline', u'considerations', u'accordingly', u'submarine', u'secure', u'representing', u'reform', u'instances', u'expert', u'comments', u'associations', u'volunteers', u'unions', u'transferred', u'suggests', u'suburban', u'regulations', u'recovery', u'proposals', u'prestige', u'perform', u'modest', u'fees', u'feeding', u'entertainment', u'dealer', u'customer', u'contributions', u'builders', u'1955', u'talents', u'supporting', u'specified', u'rendered', u'operated', u'municipal', u'meetings', u'marshall', u'maintaining', u'limitations', u'libraries', u'exclusive', u'drugs', u'diplomatic', u'communications', u'calendar', u'advantages', u'surplus', u'scope', u'sba', u'restrictions', u'requirement', u'reorganization', u'pursuant', u'permits', u'paying', u'establishing', u'employee', u'distributed', u'clinical', u'aids', u'structural', u'stockholders', u'sharing', u'revenues', u'retirement', u'quoted', u'promotion', u'productive', u'peaceful', u'instruction', u'exceptions', u'eliminate', u'assist', u'toll', u'prospect', u'probable', u'oriented', u'operational', u'lands', u'export', u'exploration', u'discrimination', u'departments', u'coverage', u'corporations', u'contracts', u'conferences', u'civilian', u'applications', u'allowances', u'adult', u'wishes', u'weekly', u'recommendation', u'prospects', u'preliminary', u'phases', u'monthly', u'insure', u'governmental', u'draft', u'automobiles', u'assembled', u'announcement', u'amendment', u'aimed', u'agriculture', u'access', u'absorbed', u'voluntary', u'taxpayers', u'stressed', u'savings', u'preservation', u'inventory', u'grants', u'encouraging', u'dedicated', u'attending', u'assessment', u'valid', u'stein', u'statistics', u'retained', u'respects', u'rehabilitation', u'refund', u'recommendations', u'participate', u'ownership', u'overseas', u'obligations', u'investigations', u'forests', u'carries', u'capabilities', u'bid', u'alaska', u'un', u'replacement', u'prospective', u'profits', u'procurement', u'pertinent', u'launched', u'judges', u'crises', u'commodities', u'availability', u'assessors', u'alliance', u'1946', u'supplement', u'solely', u'retail', u'repair', u'relating', u'receives', u'parks', u'minority', u'legislators', u'intervention', u'improvements', u'hospitals', u'expanded', u'enforcement', u'disposal', u'cooperative', u'commissioner', u'biological', u'adjustments', u'whereby', u'venture', u'timber', u'specialists', u'remainder', u'priority']\n",
      "Cluster 99 : [u'commercial']\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,100):\n",
    "    ind = np.where(km == i)\n",
    "    cluster = []\n",
    "    for j in ind[0]:\n",
    "          cluster.append(V[j])\n",
    "    print \"Cluster\", i, \":\", cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSS: \n",
    "\n",
    "For this 100 clusters, we could notice the relations between the words in a single cluster. For example, the word 'two' is in the same cluster of the word 'three' and the word 'four'. The word 'state' is in the same cluster of 'united' and 'city', etc... The relationship between in words each cluster is stronger than the relationship of words from different clusters and this shows the significance of the KMean clustering. In order to find words with the strongest relationships, we need to make sure that 'k' is properly chosen. For example, if k is too large, we could get too many clusters, though the words in a single cluster is really close, we might overlook other words which should be in the same clusters so that this KMeans is useless and tell us much less information. On the contrary, if k is too small, the number of words in a cluster is large, we could not tell much information with a very small k either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: one\n",
      "neighbors: [u'every', u'took', u'another', u'first', u'four']\n",
      "word: would\n",
      "neighbors: [u'could', u'might', u'want', u'think', u'anything']\n",
      "word: said\n",
      "neighbors: [u'know', u'tell', u'like', u'asked', u'knew']\n",
      "word: time\n",
      "neighbors: [u'day', u'play', u'days', u'around', u'long']\n",
      "word: new\n",
      "neighbors: [u'york', u'city', u'company', u'modern', u'development']\n",
      "word: could\n",
      "neighbors: [u'would', u'might', u'said', u'want', u'going']\n",
      "word: two\n",
      "neighbors: [u'three', u'four', u'several', u'six', u'five']\n",
      "word: may\n",
      "neighbors: [u'also', u'would', u'might', u'shall', u'possible']\n",
      "word: first\n",
      "neighbors: [u'second', u'next', u'last', u'every', u'home']\n",
      "word: man\n",
      "neighbors: [u'woman', u'young', u'girl', u'never', u'boy']\n",
      "word: like\n",
      "neighbors: [u'said', u'saw', u'know', u'little', u'woman']\n",
      "word: even\n",
      "neighbors: [u'seem', u'might', u'always', u'much', u'thought']\n",
      "word: made\n",
      "neighbors: [u'make', u'making', u'however', u'possible', u'act']\n",
      "word: also\n",
      "neighbors: [u'may', u'set', u'means', u'best', u'found']\n",
      "word: many\n",
      "neighbors: [u'several', u'two', u'important', u'certain', u'various']\n",
      "word: must\n",
      "neighbors: [u'would', u'cannot', u'individual', u'however', u'might']\n",
      "word: well\n",
      "neighbors: [u'course', u'good', u'never', u'best', u'think']\n",
      "word: af\n",
      "neighbors: [u'function', u'q', u'c', u'polynomial', u'p']\n",
      "word: back\n",
      "neighbors: [u'around', u'away', u'went', u'got', u'door']\n",
      "word: years\n",
      "neighbors: [u'year', u'days', u'minutes', u'months', u'five']\n"
     ]
    }
   ],
   "source": [
    "knn_dic = {}\n",
    "for i in range(0, 20):\n",
    "    knn_list = []\n",
    "    res = []\n",
    "    for j in range(0, len(vector_w)):\n",
    "        distance = spatial.distance.cosine(vec20[i], vector_w[j])\n",
    "        knn_dic[distance] = j\n",
    "        knn_list.append(distance)\n",
    "    knn_list.sort()\n",
    "    knn = knn_list[:6]\n",
    "#     print top20\n",
    "    for index in range(1, len(knn)):\n",
    "        res.append(wordList[knn_dic[knn[index]]])\n",
    "    print \"word:\", top20[i]\n",
    "    print \"neighbors:\", res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSS:\n",
    "\n",
    "Unlike the Kmeans clustering, KNN method returns back even numbers inside a group of words. Inside a single group, we notice the words are related to each other. For example, the word 'man' has neighbor such as 'boy', 'woman', 'girl', etc, which totally make sense to us and the word 'two' has its neighbors 'three', 'four', 'five', etc. Kmeans is unsupervised because the words vec has no external classification and KNN combines the classification of the K nearest points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Collaborative Filtering (50 points)\n",
    "\n",
    "In this second part, you will implement collaborative filtering on the Netflix prize dataset -- donâ€™t freak out, the provided sample dataset has only ~2000 items and ~28,000 users.\n",
    "\n",
    "As background, read the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) up to Section 2.1. Of course you can read further if you are interested, and you can also refer to the course slides for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ratings in test rating: 100478\n",
      "number of movies in test rating: 1701\n",
      "number of users in test rating: 27555\n",
      "number of ratings in training rating: 3255352\n",
      "number of movies in training rating: 1821\n",
      "number of users in training rating: 28978\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of ratings, \n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "import collections  \n",
    "import math\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "content_test = []\n",
    "with open(\"/Users/huangyian/Downloads/netflix-dataset/TestingRatings.txt\") as f:\n",
    "    for line in f:\n",
    "        inner_list = [elt.strip() for elt in line.split(',')]\n",
    "        content_test.append(inner_list)\n",
    "print \"number of ratings in test rating:\", len(content_test)\n",
    "\n",
    "movies_set_test = set()\n",
    "users_set_test = set()\n",
    "test_dictionary = collections.OrderedDict() \n",
    "\n",
    "for line in content_test:\n",
    "    users_set_test.add(line[1])\n",
    "    movies_set_test.add(line[0])\n",
    "    test_dictionary[line[1]] = test_dictionary.get(line[1],{})\n",
    "    test_dictionary[line[1]][line[0]] = line[2]\n",
    "    \n",
    "# for user_id,user_movies_rate in user_dic.iteritems():\n",
    "#     for movie_id,movie_rate in user_movie_rate.iteritems():\n",
    "#         print \"User: \",user_id,\" Movie: \",movie_id\n",
    "     \n",
    "print \"number of movies in test rating:\", len(movies_set_test)\n",
    "print \"number of users in test rating:\", len(users_set_test)\n",
    "\n",
    "content_train = []\n",
    "with open(\"/Users/huangyian/Downloads/netflix-dataset/TrainingRatings.txt\") as f:\n",
    "    for line in f:\n",
    "        inner_list = [elt.strip() for elt in line.split(',')]\n",
    "        content_train.append(inner_list)\n",
    "print \"number of ratings in training rating:\", len(content_train)\n",
    "\n",
    "users_set_train = set()\n",
    "movies_set_train = set()\n",
    "user_dic = collections.OrderedDict() \n",
    "\n",
    "for line in content_train:\n",
    "    users_set_train.add(line[1])\n",
    "    movies_set_train.add(line[0])\n",
    "\n",
    "    user_dic[line[1]] = user_dic.get(line[1],{})\n",
    "    user_dic[line[1]][line[0]] = line[2]\n",
    "    \n",
    "# for user_id,movie_id in user_dic.iteritems():\n",
    "#     for user_movie_rate in user_movie_rate:\n",
    "#         print \"User: \",user_id,\" Movie: \",movie_id\n",
    "    \n",
    "print \"number of movies in training rating:\", len(movies_set_train)\n",
    "print \"number of users in training rating:\", len(users_set_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Get movie_user_dic\n",
      "Finished Get User_vote_average_dic\n"
     ]
    }
   ],
   "source": [
    "def intersection(list1, list2):  \n",
    "    set1 = set(list1)  \n",
    "    set2 = set(list2)  \n",
    "    common = set1.intersection(set2)  \n",
    "    return list(common) \n",
    "\n",
    "numbers_users = 5000\n",
    "\n",
    "top_user_list = list(users_set_test)[:numbers_users]\n",
    "top_user_avg_dic = collections.OrderedDict() \n",
    "correlation_dic = collections.OrderedDict() \n",
    "movie_user_dic = collections.defaultdict(list)\n",
    "\n",
    "# Get the movie to user dictionary\n",
    "# Range: all lineItem in Trainning data\n",
    "# for user_id,user_movies in user_dic.iteritems():\n",
    "#     user_movies = user_dic.get(user_id)\n",
    "#     for movieID in user_movies.iterkeys():\n",
    "#         movie_user_dic[movieID].append(user_id)\n",
    "# print \"Finished Get movie_user_dic\"\n",
    "for user_id in top_user_list:\n",
    "    user_movies = user_dic.get(user_id)\n",
    "    for movieID in user_movies.iterkeys():\n",
    "        movie_user_dic[movieID].append(user_id)\n",
    "print \"Finished Get movie_user_dic\"\n",
    "        \n",
    "# Get the average vote for each user, store it in top_user_avg_dic\n",
    "# Range:  all LineItem in Trainning data\n",
    "for user_id,user_movies in user_dic.iteritems():\n",
    "#     print \"User: \",user_id,\" Movie: \",user_movies\n",
    "    user_movie_rate_sum = sum([float(x) for x in user_movies.itervalues()])\n",
    "    top_user_avg_dic[user_id] = user_movie_rate_sum/len(user_movies)\n",
    "print \"Finished Get User_vote_average_dic\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# Get the correlation between users, store it in correlation_dic\n",
    "for index1 in range(0, len(top_user_list)):\n",
    "    for index2 in range(index1 + 1, len(top_user_list)):\n",
    "        \n",
    "        user_x = list(top_user_list)[index1]\n",
    "        userx_movies = user_dic.get(user_x)\n",
    "        userx_movies_ID = [movie for movie in userx_movies.iterkeys()]\n",
    "        \n",
    "        user_y = list(top_user_list)[index2]\n",
    "        usery_movies = user_dic.get(user_y)\n",
    "        usery_movies_ID = [movie for movie in usery_movies.iterkeys()]\n",
    "        \n",
    "        common_movies = intersection(userx_movies_ID, usery_movies_ID)\n",
    "\n",
    "        divisor = sum([(float(userx_movies.get(movie_id))-top_user_avg_dic[user_x])\n",
    "                       *(float(usery_movies.get(movie_id))-top_user_avg_dic[user_y]) \n",
    "                       for movie_id in common_movies])\n",
    "    \n",
    "        division = np.sqrt(sum([np.square(float(userx_movies.get(movie_id))-top_user_avg_dic[user_x]) for movie_id in common_movies]) *\n",
    "                  sum([np.square(float(usery_movies.get(movie_id))-top_user_avg_dic[user_y]) for movie_id in common_movies]))\n",
    "        \n",
    "        correlation = divisor/division if divisor != 0 else 0\n",
    "#         print \"Divisor is: \",divisor, \" division is:\", division, \" correlation is: \",correlation\n",
    "        \n",
    "        correlation_dic.setdefault(user_x, {})[user_y] = correlation\n",
    "        correlation_dic.setdefault(user_y, {})[user_x] = correlation\n",
    "    \n",
    "print \"end\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.744302083101\n",
      "Root Mean Squared Error: 0.9393061290821592\n"
     ]
    }
   ],
   "source": [
    "k = 0.001\n",
    "predict_dict = collections.defaultdict(list)\n",
    "count = 0\n",
    "deltasum = 0\n",
    "deltaSquareSum = 0\n",
    "\n",
    "for index in range(0, 5000):\n",
    "    user_id = top_user_list[index]\n",
    "    movies_rates = test_dictionary.get(user_id)\n",
    "    for movie_id, movie_rate in movies_rates.iteritems():\n",
    "#         print \"User \",user_id,\"'s rate to movie \",movie_id,\"is\",movie_rate,\" His average score is\",top_user_avg_dic.get(user_id)\n",
    "        predict_score = top_user_avg_dic.get(user_id)    \n",
    "        if not movie_user_dic.get(movie_id):\n",
    "            predict_score = top_user_avg_dic.get(user_id)\n",
    "        else:\n",
    "            predict_score = top_user_avg_dic.get(user_id) + k * sum([(float(user_dic.get(activity_user).get(movie_id)) - top_user_avg_dic.get(activity_user)) * float(correlation_dic.get(user_id).get(activity_user)) \n",
    "                                                                     for activity_user in movie_user_dic.get(movie_id)])\n",
    "        deltasum += abs(predict_score-float(movie_rate))\n",
    "        deltaSquareSum += np.square(predict_score-float(movie_rate))\n",
    "        count += 1\n",
    "    \n",
    "#         predict_dict.setdefault(user_id, {})[movie_id] = predict_score\n",
    "\n",
    "print \"Mean Absolute Error:\", deltasum/count\n",
    "print \"Root Mean Squared Error:\", np.sqrt(deltaSquareSum/count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of k = 0.001: 0.744302083101\n",
      "Root Mean Squared Error of k = 0.001: 0.9393061290821592\n",
      "Mean Absolute Error of k = 0.01: 1.13569500504\n",
      "Root Mean Squared Error of k = 0.01: 1.607518858361435\n",
      "Mean Absolute Error of k = 0.1: 9.92803887345\n",
      "Root Mean Squared Error of k = 0.1: 16.438431669880522\n"
     ]
    }
   ],
   "source": [
    "# change the value of k\n",
    "k1 = 0.01\n",
    "k2 = 0.1\n",
    "predict_dict_k1 = collections.defaultdict(list)\n",
    "count_k1 = 0\n",
    "deltasum_k1 = 0\n",
    "deltaSquareSum_k1 = 0\n",
    "predict_dict_k2 = collections.defaultdict(list)\n",
    "count_k2 = 0\n",
    "deltasum_k2 = 0\n",
    "deltaSquareSum_k2 = 0\n",
    "\n",
    "for index in range(0, 5000):\n",
    "    user_id = top_user_list[index]\n",
    "    movies_rates = test_dictionary.get(user_id)\n",
    "    for movie_id, movie_rate in movies_rates.iteritems():\n",
    "#         print \"User \",user_id,\"'s rate to movie \",movie_id,\"is\",movie_rate,\" His average score is\",top_user_avg_dic.get(user_id)\n",
    "        predict_score_k1 = top_user_avg_dic.get(user_id) \n",
    "        predict_score_k2 = top_user_avg_dic.get(user_id) \n",
    "        if not movie_user_dic.get(movie_id):\n",
    "            predict_score_k1 = top_user_avg_dic.get(user_id)\n",
    "            predict_score_k2 = top_user_avg_dic.get(user_id)\n",
    "        else:\n",
    "            predict_score_k1 = top_user_avg_dic.get(user_id) + k1 * sum([(float(user_dic.get(activity_user).get(movie_id)) - top_user_avg_dic.get(activity_user)) * float(correlation_dic.get(user_id).get(activity_user)) \n",
    "                                                                     for activity_user in movie_user_dic.get(movie_id)])\n",
    "            predict_score_k2 = top_user_avg_dic.get(user_id) + k2 * sum([(float(user_dic.get(activity_user).get(movie_id)) - top_user_avg_dic.get(activity_user)) * float(correlation_dic.get(user_id).get(activity_user)) \n",
    "                                                                     for activity_user in movie_user_dic.get(movie_id)])\n",
    "        deltasum_k1 += abs(predict_score_k1-float(movie_rate))\n",
    "        deltaSquareSum_k1 += np.square(predict_score_k1-float(movie_rate))\n",
    "        count_k1 += 1\n",
    "        deltasum_k2 += abs(predict_score_k2-float(movie_rate))\n",
    "        deltaSquareSum_k2 += np.square(predict_score_k2-float(movie_rate))\n",
    "        count_k2 += 1\n",
    "    \n",
    "#         predict_dict.setdefault(user_id, {})[movie_id] = predict_score\n",
    "print \"Mean Absolute Error of k = 0.001:\", deltasum/count\n",
    "print \"Root Mean Squared Error of k = 0.001:\", np.sqrt(deltaSquareSum/count)\n",
    "\n",
    "print \"Mean Absolute Error of k = 0.01:\", deltasum_k1/count_k1\n",
    "print \"Root Mean Squared Error of k = 0.01:\", np.sqrt(deltaSquareSum_k1/count_k1)\n",
    "\n",
    "print \"Mean Absolute Error of k = 0.1:\", deltasum_k2/count_k2\n",
    "print \"Root Mean Squared Error of k = 0.1:\", np.sqrt(deltaSquareSum_k2/count_k2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "top_user = list(users_set_test)[:num]\n",
    "correlation_cosine = collections.OrderedDict() \n",
    "for index1 in range(0, len(top_user)):\n",
    "    for index2 in range(index1 + 1, len(top_user)):\n",
    "        \n",
    "        user_x = list(top_user)[index1]\n",
    "        userx_movies = user_dic.get(user_x)\n",
    "        userx_movies_ID = [movie for movie in userx_movies.iterkeys()]\n",
    "        \n",
    "        user_y = list(top_user_list)[index2]\n",
    "        usery_movies = user_dic.get(user_y)\n",
    "        usery_movies_ID = [movie for movie in usery_movies.iterkeys()]\n",
    "        \n",
    "        common_movies = intersection(userx_movies_ID, usery_movies_ID)\n",
    "\n",
    "        numerator = sum([(float(userx_movies.get(movie_id)))*(float(usery_movies.get(movie_id))) for movie_id in common_movies])\n",
    "\n",
    "        \n",
    "    \n",
    "        denominator1 = np.sqrt(sum([np.square(float(userx_movies.get(movie_id))) for movie_id in userx_movies_ID]))\n",
    "        denominator2 = np.sqrt(sum([np.square(float(usery_movies.get(movie_id))) for movie_id in usery_movies_ID]))\n",
    "        denominator = denominator1 * denominator2 \n",
    "        correlation = numerator/(denominator) if denominator != 0 else 0\n",
    "#         print \"Divisor is: \",divisor, \" division is:\", division, \" correlation is: \",correlation\n",
    "        \n",
    "        correlation_cosine.setdefault(user_x, {})[user_y] = correlation\n",
    "        correlation_cosine.setdefault(user_y, {})[user_x] = correlation\n",
    "    \n",
    "print \"end\"\n",
    "# print correlation_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity:\n",
      "Mean Absolute Error: 0.730877571498\n",
      "Root Mean Squared Error: 0.9110322978049715\n"
     ]
    }
   ],
   "source": [
    "predict = collections.defaultdict(list)\n",
    "c = 0\n",
    "d_sum = 0\n",
    "squareSum = 0\n",
    "movie_user = collections.defaultdict(list)\n",
    "\n",
    "for user_id in top_user:\n",
    "    user_movies = user_dic.get(user_id)\n",
    "    for movieID in user_movies.iterkeys():\n",
    "        movie_user[movieID].append(user_id)\n",
    "for index in range(0, num):\n",
    "    user_id = top_user[index]\n",
    "    movies_rates = test_dictionary.get(user_id)\n",
    "    for movie_id, movie_rate in movies_rates.iteritems():\n",
    "#         print \"User \",user_id,\"'s rate to movie \",movie_id,\"is\",movie_rate,\" His average score is\",top_user_avg_dic.get(user_id)\n",
    "        predict_score = top_user_avg_dic.get(user_id)    \n",
    "        if not movie_user.get(movie_id):\n",
    "            predict = top_user_avg_dic.get(user_id)\n",
    "        else:\n",
    "#             print float(correlation_cosine.get(user_id).get(activity_user))\n",
    "            predict = top_user_avg_dic.get(user_id) + k * sum([float(user_dic.get(activity_user).get(movie_id)) - top_user_avg_dic.get(activity_user) * float(correlation_cosine.get(user_id).get(activity_user)) for activity_user in movie_user.get(movie_id)])\n",
    "        d_sum += abs(predict_score-float(movie_rate))\n",
    "        squareSum += np.square(predict_score-float(movie_rate))\n",
    "        c += 1\n",
    "    \n",
    "#         predict_dict.setdefault(user_id, {})[movie_id] = predict_score\n",
    "print \"cosine similarity:\"\n",
    "print \"Mean Absolute Error:\", d_sum/c\n",
    "print \"Root Mean Squared Error:\", np.sqrt(squareSum/c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSS:\n",
    "    \n",
    "The prediction of a vote between an active user and a single movie is related to 'k', the correlation matrix, and the average of the user's votes. \n",
    "\n",
    "In order to improve RMSE and MAE, I decide to change those factors, in this case, the value of 'k' and the correlation matrix. When using k = 0.001, the MAE and RMSE reach the minimum which indicates the best results, however, when applying k = 0.01 and k = 0.1, RMSE and MAE accumulates which indicates the larger prediction error, which told us k should be chosen in considerate of the rate range of the scoring system. When applying k = 0.1 and 0.01, the range will not be within 0~5, some of the values exceed the range. \n",
    "\n",
    "To deal with this problem, we could either change the value of k or distort the prediction votes value a bit to meet the scoring requirement.\n",
    "\n",
    "Also, we could use different method to calculate the correlation matrix. In this case, I changed the Pearson method to cosine similarity. Comparing with the Pearson, the mae decreased from 0.744 to 0.731, and RMSE decreases from 0.939 to 0.911."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
